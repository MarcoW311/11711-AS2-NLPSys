{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2e32e7-40c0-4a32-9f5b-db5be1ea65df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjing2/capstone_venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88002e57-0d4b-4d9c-b762-51b12fe03627",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722b3ef-5b8d-4ff3-8b98-a9d72a182f16",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12b5fc0-f771-42de-bb68-6de29b4c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(file):\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.read()\n",
    "    \n",
    "    sentences = lines.split('\\n\\n')\n",
    "    all_sent = []\n",
    "    all_tag = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        rows = sent.split(\"\\n\")\n",
    "        words = []\n",
    "        tags = []\n",
    "        \n",
    "        for r in rows[:-1]:\n",
    "            w, t = r.split(\" \")\n",
    "            words.append(w)\n",
    "            tags.append(t)\n",
    "    \n",
    "        all_sent.append(words[:])\n",
    "        all_tag.append(tags[:])\n",
    "\n",
    "    return pd.DataFrame({\"tokens\":all_sent, \"ner_tags\":all_tag})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed52f005-72a6-4fe2-83a1-b267aa231f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "\n",
    "for i in [1,2,4,5]:\n",
    "    file = \"acl-out{}.conll\".format(i)\n",
    "    if train_df is None:\n",
    "        train_df = create_dataset(file)\n",
    "    else:\n",
    "        train_df = pd.concat([train_df, create_dataset(file)], ignore_index=True)\n",
    "\n",
    "val_df = create_dataset(\"acl-out3.conll\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1086c4-6d9d-4051-9c1c-b1d175e7e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_private.csv', index_col=\"id\", skip_blank_lines=False)\n",
    "inputs = test_data[\"input\"].to_list()\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    if pd.isna(inputs[i]):\n",
    "        inputs[i] = \"END\"\n",
    "\n",
    "test_data[\"input\"] = inputs\n",
    "test_data['target'] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d479feca-0d88-4f2c-9866-305737791c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_tags = []\n",
    "test_words = test_data['input'].to_list()\n",
    "\n",
    "sentence = []\n",
    "for word in test_words[1:]:\n",
    "    if word == \"END\":\n",
    "        sentence.append(word)\n",
    "        test_texts.append(sentence[:])\n",
    "        test_tags.append([\"O\"]*len(sentence))\n",
    "        sentence = []\n",
    "    else:\n",
    "        sentence.append(word)\n",
    "sentence.append(\"END\")\n",
    "\n",
    "test_texts.append(sentence[:])\n",
    "test_tags.append([\"O\"]*len(sentence))\n",
    "\n",
    "test_df = pd.DataFrame({\"tokens\":test_texts, \"ner_tags\":test_tags})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2a154-6cb7-4a57-ac9a-f50577df694a",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3223ec80-08db-4cf3-88f9-de31b7f8a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "labels = set()\n",
    "for i, r in train_df.iterrows():    \n",
    "    labels = labels.union(set(r['ner_tags']))\n",
    "\n",
    "tag2id = {tag: id for id, tag in enumerate(labels)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c76fc2-fd58-4247-8494-029762f035a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['tokens'].to_list()\n",
    "val_texts = val_df['tokens'].to_list()\n",
    "test_texts = test_df['tokens'].to_list()\n",
    "\n",
    "train_tags = train_df['ner_tags'].to_list()\n",
    "val_tags = val_df['ner_tags'].to_list()\n",
    "test_tags = test_df['ner_tags'].to_list()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f96413-4650-4456-8282-d5e0615e9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings, test=False):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        if test:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = tag2id[\"O\"]\n",
    "        else:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)\n",
    "test_labels = encode_tags(test_tags, test_encodings, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38170a1c-6e09-45ed-91c2-7623f8aecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "test_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = PaperDataset(train_encodings, train_labels)\n",
    "val_dataset = PaperDataset(val_encodings, val_labels)\n",
    "test_dataset = PaperDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32500971-7cb3-4c6f-955a-80757164b0bf",
   "metadata": {},
   "source": [
    "# Fine-tuning with trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61c321-b0f0-4278-99b8-bc664345ec49",
   "metadata": {},
   "source": [
    "## Setting Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34330727-9b39-4ad0-81c1-d9df6dce8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22edc4-e5b8-4a85-9ede-699b479b59bd",
   "metadata": {},
   "source": [
    "## Setting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424fa1d2-0056-4cb6-9423-0caa2183cc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1416806/848863643.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "metric.compute(predictions=[labels], references=[labels])\n",
    "label_list = list(labels)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b134d-3eb3-4a5d-b206-7c9d5c02f063",
   "metadata": {},
   "source": [
    "## Setting Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca14db5-092c-4aae-b02f-c58f6abf187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # data_collator=data_collator,\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5235f3-1c43-4a5a-ab5c-d5007831fd99",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2271b618-4bbb-4c07-9ebd-d6aa24fe34af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 10:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.440100</td>\n",
       "      <td>1.833861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.680808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.595269</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.881164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.483594</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.888440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.360613</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.331034</td>\n",
       "      <td>0.898949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.297747</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.532544</td>\n",
       "      <td>0.920776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.329308</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.449704</td>\n",
       "      <td>0.911884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.339178</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.573099</td>\n",
       "      <td>0.927243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.348912</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.537143</td>\n",
       "      <td>0.924818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.356296</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.620321</td>\n",
       "      <td>0.924010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjing2/capstone_venv/lib64/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zjing2/capstone_venv/lib64/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=260, training_loss=0.49375794031299075, metrics={'train_runtime': 629.6484, 'train_samples_per_second': 6.448, 'train_steps_per_second': 0.413, 'total_flos': 408232295062200.0, 'train_loss': 0.49375794031299075, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca92166-741c-4ab0-aafd-d46e431d3653",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e72aeeb-7f7b-49f6-91e1-a39c11f09343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35629627108573914,\n",
       " 'eval_precision': 0.6666666666666666,\n",
       " 'eval_recall': 0.58,\n",
       " 'eval_f1': 0.6203208556149732,\n",
       " 'eval_accuracy': 0.9240097008892482,\n",
       " 'eval_runtime': 0.9665,\n",
       " 'eval_samples_per_second': 38.282,\n",
       " 'eval_steps_per_second': 1.035,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364a60c-e14e-4c77-b8d0-f3bcddaca314",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625e1337-f82b-4ac6-8cb7-3c2b0da2b0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c215528-a822-4350-81a7-82fdd470bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "groundtruth = []\n",
    "for prediction, label in zip(predictions, labels):\n",
    "    level_p = []\n",
    "    level_l = []\n",
    "    for p, l in zip(prediction, label):\n",
    "        # print(p, l)\n",
    "        if l != -100 and p != -100:\n",
    "            # print(p, id2tag[p], l, id2tag[l])\n",
    "            level_p.append(id2tag[p])\n",
    "            level_l.append(id2tag[l])\n",
    "    predict.append(level_p)\n",
    "    groundtruth.append(level_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "351bf8f8-e624-4669-9046-55bbd2455d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set DocString\n",
    "res = [\"O\"]\n",
    "# Add \"O\" to sentence that exceed MAX_LENGTH of the pre-trained model, which is 512 tokens.\n",
    "predict[676] += [\"O\"]*57\n",
    "predict[466] += [\"O\"]*3\n",
    "\n",
    "for i in range(len(predict)):\n",
    "    res += predict[i][:]\n",
    "\n",
    "# Remove the last END\n",
    "res.pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fafa4-35e8-4901-b267-5b1a50f3c688",
   "metadata": {},
   "source": [
    "## Output Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c6b87d-cac8-4dfe-9c19-1e6fffeb0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['target'] = res\n",
    "# test_data.drop(columns=['input']).to_csv(\"test_res.csv\")\n",
    "# test_data.to_csv(\"test_res_with_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec160af-af54-4f20-8c0b-a67f2ef5b80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
