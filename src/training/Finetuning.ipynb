{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e32e7-40c0-4a32-9f5b-db5be1ea65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88002e57-0d4b-4d9c-b762-51b12fe03627",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722b3ef-5b8d-4ff3-8b98-a9d72a182f16",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b5fc0-f771-42de-bb68-6de29b4c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(file):\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.read()\n",
    "    \n",
    "    sentences = lines.split('\\n\\n')\n",
    "    all_sent = []\n",
    "    all_tag = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        rows = sent.split(\"\\n\")\n",
    "        words = []\n",
    "        tags = []\n",
    "        \n",
    "        for r in rows[:-1]:\n",
    "            w, t = r.split(\" \")\n",
    "            words.append(w)\n",
    "            tags.append(t)\n",
    "    \n",
    "        all_sent.append(words[:])\n",
    "        all_tag.append(tags[:])\n",
    "\n",
    "    return pd.DataFrame({\"tokens\":all_sent, \"ner_tags\":all_tag})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52f005-72a6-4fe2-83a1-b267aa231f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "\n",
    "for i in [1,2,4,5]:\n",
    "    file = \"acl-out{}.conll\".format(i)\n",
    "    if train_df is None:\n",
    "        train_df = create_dataset(file)\n",
    "    else:\n",
    "        train_df = pd.concat([train_df, create_dataset(file)], ignore_index=True)\n",
    "\n",
    "val_df = create_dataset(\"acl-out3.conll\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1086c4-6d9d-4051-9c1c-b1d175e7e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_private.csv', index_col=\"id\", skip_blank_lines=False)\n",
    "inputs = test_data[\"input\"].to_list()\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    if pd.isna(inputs[i]):\n",
    "        inputs[i] = \"END\"\n",
    "\n",
    "test_data[\"input\"] = inputs\n",
    "test_data['target'] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479feca-0d88-4f2c-9866-305737791c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_tags = []\n",
    "test_words = test_data['input'].to_list()\n",
    "\n",
    "sentence = []\n",
    "for word in test_words[1:]:\n",
    "    if word == \"END\":\n",
    "        sentence.append(word)\n",
    "        test_texts.append(sentence[:])\n",
    "        test_tags.append([\"O\"]*len(sentence))\n",
    "        sentence = []\n",
    "    else:\n",
    "        sentence.append(word)\n",
    "sentence.append(\"END\")\n",
    "\n",
    "test_texts.append(sentence[:])\n",
    "test_tags.append([\"O\"]*len(sentence))\n",
    "\n",
    "test_df = pd.DataFrame({\"tokens\":test_texts, \"ner_tags\":test_tags})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2a154-6cb7-4a57-ac9a-f50577df694a",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223ec80-08db-4cf3-88f9-de31b7f8a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "labels = set()\n",
    "for i, r in train_df.iterrows():    \n",
    "    labels = labels.union(set(r['ner_tags']))\n",
    "\n",
    "tag2id = {tag: id for id, tag in enumerate(labels)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76fc2-fd58-4247-8494-029762f035a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['tokens'].to_list()\n",
    "val_texts = val_df['tokens'].to_list()\n",
    "test_texts = test_df['tokens'].to_list()\n",
    "\n",
    "train_tags = train_df['ner_tags'].to_list()\n",
    "val_tags = val_df['ner_tags'].to_list()\n",
    "test_tags = test_df['ner_tags'].to_list()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f96413-4650-4456-8282-d5e0615e9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings, test=False):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        if test:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = tag2id[\"O\"]\n",
    "        else:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)\n",
    "test_labels = encode_tags(test_tags, test_encodings, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38170a1c-6e09-45ed-91c2-7623f8aecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "test_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = PaperDataset(train_encodings, train_labels)\n",
    "val_dataset = PaperDataset(val_encodings, val_labels)\n",
    "test_dataset = PaperDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32500971-7cb3-4c6f-955a-80757164b0bf",
   "metadata": {},
   "source": [
    "# Fine-tuning with trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61c321-b0f0-4278-99b8-bc664345ec49",
   "metadata": {},
   "source": [
    "## Setting Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34330727-9b39-4ad0-81c1-d9df6dce8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=25,              # total number of training epochs\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22edc4-e5b8-4a85-9ede-699b479b59bd",
   "metadata": {},
   "source": [
    "## Setting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fa1d2-0056-4cb6-9423-0caa2183cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "metric.compute(predictions=[labels], references=[labels])\n",
    "label_list = list(labels)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b134d-3eb3-4a5d-b206-7c9d5c02f063",
   "metadata": {},
   "source": [
    "## Setting Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca14db5-092c-4aae-b02f-c58f6abf187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # data_collator=data_collator,\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5235f3-1c43-4a5a-ab5c-d5007831fd99",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271b618-4bbb-4c07-9ebd-d6aa24fe34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca92166-741c-4ab0-aafd-d46e431d3653",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72aeeb-7f7b-49f6-91e1-a39c11f09343",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364a60c-e14e-4c77-b8d0-f3bcddaca314",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c215528-a822-4350-81a7-82fdd470bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "predict = []\n",
    "for prediction in predictions:\n",
    "    level_p = []\n",
    "    for p in prediction:\n",
    "        if p != -100:\n",
    "            level_p.append(id2tag[p])\n",
    "    predict.append(level_p)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bf8f8-e624-4669-9046-55bbd2455d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DocString\n",
    "res = [\"O\"]\n",
    "# Add \"O\" to sentence that exceed MAX_LENGTH of the pre-trained model, which is 512 tokens.\n",
    "predict[676] += [\"O\"]*57\n",
    "predict[466] += [\"O\"]*3\n",
    "\n",
    "for i in range(len(predict)):\n",
    "    res += predict[i][:]\n",
    "    temp += len(test_df['tokens'][i])\n",
    "# Remove the last END\n",
    "res.pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fafa4-35e8-4901-b267-5b1a50f3c688",
   "metadata": {},
   "source": [
    "## Output Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6b87d-cac8-4dfe-9c19-1e6fffeb0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['target'] = res\n",
    "test_data.drop(columns=['input']).to_csv(\"test_res.csv\")\n",
    "test_data.to_csv(\"test_res_with_input.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
