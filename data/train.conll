Leveraging O
Multiple B-MethodName
User I-MethodName
Simulators I-MethodName
to O
train O
Task O
- O
oriented O
Dialogue O
Systems O
Yajiao O
LIU1,2 O
, O
Xin O
Jiang3 O
, O
Yichun O
Yin3 O
, O
Yasheng O
Wang3 O
, O
Fei O
Mi3 O
Qun O
Liu3 O
, O
Xiang O
Wan2 O
, O
Benyou O
Wang1,2 O
1The O
Chinese O
University O
of O
Hong O
Kong O
, O
Shenzhen O
2Shenzhen O
Research O
Institute O
of O
Big O
Data O
3Huawei O
Noah O
’s O
Ark O
Lab O
yajiaoliu O
@ O
link.cuhk.edu.cn O

In O
this O
paper O
, O
we O
propose O
a O
framework O
called O
MUST1to B-MethodName
optimize I-MethodName
ToD I-MethodName
systems O
via O
leveraging O
Multiple O
UserSimula O
Tors O
. O

To O
tackle O
these O
challenges O
, O
we O
formulate O
MUST O
as O
a O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandits I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
and O
provide O
a O
method O
called O
MUST B-MethodName
adaptive I-MethodName
that O
balances O
i O
) O
the O
boosting O
adaption O
for O
adaptive O
interactions O
between O
different O
user O
simulators O
and O
the O
ToD O
system O
andii O
) O
the O
uniform O
adaption O
to O
avoid O
the O
catastrophic O
forgetting O
issue O
. O

With O
both O
automatic O
evaluations O
and O
human O
evaluations O
, O
our O
experimental O
results O
on O
MultiWOZ B-DatasetName
show O
that O
the O
dialogue O
system O
trained O
by O
MUST B-MethodName
achieves O
a O
better O
performance O
than O
those O
trained O
by O
a O
single O
user O
simulator O
. O

By O
using O
a O
user O
simulator O
and O
sampling O
user O
goals O
, O
we O
can O
train O
the O
dialogue O
system O
from O
scratch O
with O
reinforcement B-MethodName
learning I-MethodName
( O
RL O
) O
algorithms O
. O

( O
2019 O
) O
builds O
various O
user O
simulators O
and O
analyzes O
the O
behavior O
of O
each O
user O
simulator O
in O
the O
popular O
restaurant B-TaskName
search I-TaskName
task O
from O
MultiWOZ B-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018 O
) O
. O

In O
this O
paper O
, O
we O
propose O
a O
framework O
called O
MUST B-MethodName
to O
utilize O
Multiple O
UserSimula O
Tors O
simultaneously O
to O
obtain O
a O
better O
system O
agent O
. O

There O
exist O
several O
simple O
ways O
to O
implement O
the O
MUST B-MethodName
framework O
, O
including O
a O
merging O
strategy O
, O
a O
continual O
reinforcement O
learning O
( O
CRL O
) O
strategy O
, O
and O
a O
uniform O
adaption O
strategy O
, O
namely O
MUST B-MethodName
merging I-MethodName
, O
MUST B-MethodName
CRL I-MethodName
, O
and O
MUST B-MethodName
uniform I-MethodName
respectively O
( O
See O
§ O
3.2 O
) O
. O

To O
tackle O
them O
effectively O
, O
we O
first O
formulate O
the O
problem O
as O
a O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandits I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
( O
Auer O
et O
al O
. O
, O
2002 O
) O
; O
similar O
to O
the O
exploitation O
vs O
exploration O
trade O
- O
off O
, O
specifying O
multiple O
user O
simulators O
should O
trade O
off O
a O
boosting O
adaption O
( O
tackling O
challenge O
1 O
) O
and O
a O
uniform O
adaption O
( O
tackling O
challenge O
2 O
) O
, O
see O
§ O
4.1 O
for O
more O
details O
. O

Then O
we O
implement O
a O
new O
method O
called O
MUST B-MethodName
adaptive O
to O
utilize O
an O
adaptively O
- O
updated O
distribution O
among O
all O
user O
simulators O
to O
sample O
them O
when O
training O
the O
dialogue O
system O
in O
the O
RL O
training O
. O

Our O
contributions O
are O
three O
- O
fold O
: O
( O
1 O
) O
To O
the O
best O
of O
our O
knowledge O
, O
our O
proposed O
MUST B-MethodName
is O
the O
first O
developed O
work O
to O
improve O
the O
dialogue O
system O
by O
using O
multiple O
user O
simulators O
simultaneously O
; O
( O
2 O
) O
We O
design O
several O
ways O
to O
implement O
the O
MUST B-MethodName
. O

Especially O
, O
we O
formulate O
MUST O
as O
a O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandits I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
, O
based O
on O
which O
we O
provide O
a O
novel O
method O
MUST B-MethodName
adaptive I-MethodName
; O
and O
( O
3 O
) O
The O
results O
show O
that O
dialogue O
systems O
trained O
with O
MUST B-MethodName
consistently O
outperform O
those O
trained O
with O
a O
single O
user O
simulator O
through O
automatic O
and O
human O
evaluations O
, O
showing O
its O
potential O
for O
robustness O
to O
the O
diversity O
of O
user O
simulators O
. O

Moreover O
, O
our O
results O
show O
that O
our O
method O
MUST B-MethodName
adaptive O
can O
efficiently O
leverage O
multiple O
user O
simulators O
to O
train O
the O
dialogue O
system O
in O
terms O
of O
convergence B-MetricName
speed I-MetricName
. O

Task O
- O
oriented O
dialogue O
systems O
aim O
to O
help O
users O
accomplish O
various O
tasks O
such O
as O
restaurant O
reservations O
through O
natural B-TaskName
language I-TaskName
conversations I-TaskName
. O

3 O
MUST B-MethodName
: O
a O
Framework O
to O
Leverage O
Multiple O
User O
SimulaTors O
3.1 O
Motivations O
to O
Use O
Multiple O
Simulators O
User O
simulators O
behave O
differently O
. O

3.2 O
Some O
Preliminary O
Proposals O
for O
MUST B-MethodName
We O
propose O
a O
framework O
called O
MUST B-MethodName
, O
the O
core O
idea O
of O
which O
is O
to O
train O
a O
better O
dialogue O
system O
by O
leveraging O
Multiple O
UserSimula O
Tors O
simultaneously O
. O

There O
are O
several O
simple O
ways O
to O
implement O
our O
MUST B-MethodName
, O
including O
a O
merging O
strategy O
( O
MUST B-MethodName
merging I-MethodName
) O
, O
aContinual O
Reinforcement O
Learningstrategy O
( O
MUST B-MethodName
CRL I-MethodName
) O
, O
and O
a O
uniform O
adaption O
strategy O
( O
MUST B-MethodName
uniform I-MethodName
) O
. O

( O
I O
) O
MUST B-MethodName
merging O
first O
samples O
some O
dialogues O
from O
each O
user O
simulator O
and O
the O
corresponding O
dialogue O
system O
trained O
by O
this O
simulator O
. O

( O
II O
) O
MUST B-MethodName
CRL5treats O
each O
user O
simulator O
as O
an O
independent O
RL O
environment O
. O

( O
III O
) O
MUST B-MethodName
uniform I-MethodName
allows O
the O
system O
agent O
have O
chances O
to O
interact O
with O
all O
user O
simulators O
simultaneously O
. O

Different O
from O
MUST B-MethodName
CRL I-MethodName
, O
MUST B-MethodName
uniform I-MethodName
puts O
all O
user O
simulators O
in O
a O
single O
RL O
environment O
and O
adopts O
the O
simplest O
way O
to O
specify O
different O
user O
simulators O
to O
train O
the O
dialogue O
system O
, O
which O
is O
to O
pick O
a O
user O
simulator O
among O
all O
user O
simulators O
with O
a O
uniform O
distribution O
for O
each O
iteration O
in O
the O
RL O
training O
. O

It O
is O
difficult O
to O
adaptively O
adjust O
weights O
of O
user O
simulators O
during O
training O
in O
MUST B-MethodName
merging I-MethodName
. O

Since O
the O
proportions O
of O
dialogues O
from O
each O
user O
simulator O
are O
fixed O
in O
MUST B-MethodName
merging I-MethodName
, O
user O
simulators O
might O
be O
well O
- O
adapted O
and O
others O
might O
not O
. O

The O
MUST B-MethodName
CRL I-MethodName
strategy O
has O
a O
problem O
of O
catastrophic O
forgetting O
( O
Khetarpal O
et O
al O
. O
, O
2020 O
) O
and O
would O
be O
sensitive O
to O
the O
order O
of O
different O
user O
agents O
interacting O
with O
the O
dialogue O
system O
, O
which O
might O
result O
in O
obtaining O
a O
sub O
- O
optimal O
dialogue O
system O
. O

A O
uniform O
distribution O
for O
the O
simulator O
selection O
under O
MUST B-MethodName
uniform O
will O
result O
in O
inefficient O
training O
, O
since O
it O
would O
be O
unnecessary O
to O
assign O
the O
many O
training O
costs O
for O
easily O
- O
adapted O
user O
simulators O
. O

Overall O
, O
the O
challenging O
problems O
under O
MUST B-MethodName
are O
1 O
) O
how O
to O
efficiently O
leverage O
multiple O
user O
simulators O
to O
train O
the O
system O
agent O
, O
and O
2 O
) O
avoiding O
the O
catastrophic O
forgetting O
issue O
. O

4 O
MUST B-MethodName
as O
a O
MAB B-TaskName
Problem O
To O
tackle O
the O
challenges O
in O
MUST B-MethodName
, O
we O
first O
formulate O
MUST B-MethodName
as O
a O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandit I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
, O
see O
§ O
4.1 O
. O

In O
§ O
4.2 O
, O
we O
propose O
a O
method O
called O
MUST B-MethodName
adaptive O
to O
use O
an O
adaptively O
- O
updated O
distribution O
to O
replace O
the O
uniform O
distribution O
under O
the O
MUST B-MethodName
uniform O
for O
accelerating O
the O
MUST O
training O
. O

We O
briefly O
compare O
these O
different O
implementations O
of O
MUST B-MethodName
in O
Tab O
. O

4.1 O
Formulating O
MUST B-MethodName
as O
a O
MAB B-TaskName
Problem I-TaskName
Adaptively O
specifying O
user O
simulators O
to O
train O
dialogue O
systems O
reminds O
us O
of O
a O
similar O
concept O
in O
machine O
learning O
, O
called O
boosting O
( O
Zhou O
, O
2012 O
) O
. O

In O
MUST B-MethodName
, O
we O
accordingly O
assume O
that O
it O
should O
reduce O
the O
interactions O
between O
the O
dialogue O
system O
and O
those O
user O
simulators O
that O
the O
system O
has O
performed O
well O
; O
and O
meanwhile O
increase O
the O
interactions O
between O
the O
system O
and O
other O
user O
simulators O
that O
the O
system O
performs O
poorly O
. O

Such O
a O
trade O
- O
off O
between O
boosting O
adaption O
and O
uniform O
adaption O
is O
similar O
to O
the O
the O
exploitation O
vs O
exploration O
trade O
- O
off O
existing O
in O
the O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandit I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
( O
Auer O
et O
al O
. O
, O
2002 O
) O
. O

Here O
, O
we O
interpret O
MUST B-MethodName
as O
a O
MAB B-TaskName
problem O
. O

In O
MUST B-MethodName
, O
the O
reward O
received O
in O
each O
armpulling O
step O
refers O
to O
the O
possible O
performance O
gain O
of O
the O
dialogue O
system O
after O
it O
interacts O
with O
a O
selected O
user O
simulator O
. O

A O
significant O
difference O
between O
the O
standard O
MAB B-TaskName
problem O
and O
MUST B-MethodName
is O
that O
the O
reward O
expectation O
of O
a O
user O
simulator O
( O
arm O
) O
in O
MUST B-MethodName
is O
not O
static O
; O
it O
changes O
over O
time O
. O

For O
example O
, O
by O
consecutively O
interacting O
with O
the O
same O
user O
simulator O
, O
the O
performance O
gain O
( O
reward B-MetricName
) O
of O
the O
system O
will O
decay O
since O
the O
system O
might O
be O
in O
saturation O
or O
overfitting O
to O
this O
simulator O
. O

To O
deal O
with O
this O
difference O
, O
we O
should O
tailor O
the O
solution O
of O
MAB B-TaskName
to O
the O
MUST B-MethodName
framework O
. O

4.2 O
Training O
with O
MUST B-MethodName
adaptive O
To O
solve O
this O
MAB B-TaskName
problem O
in O
MUST B-MethodName
, O
we O
implement O
a O
method O
called O
MUST B-MethodName
adaptive O
with O
a O
two O
- O
phase O
procedure O
, O
as O
presented O
in O
Algorithm O
1 O
. O

MUST B-MethodName
adaptive O
specifies O
user O
simulators O
in O
a4Algorithm O
1 O
: O
Implement O
MUST B-MethodName
adaptive O
with O
the O
modified O
UCB1 O
algorithm O
Input O
: O
K B-HyperparameterName
fixed O
User O
simulators O
U= O
{ O
U1 O
, O
U2 O
, O
···UK O
} O
and O
the O
values O
of O
hyperparameters O
Twarmup B-HyperparameterName
, O
T B-HyperparameterName
, O
e B-HyperparameterName
, O
d B-HyperparameterName
, O
τ B-HyperparameterName
; O
1Initialization O
: O
randomly O
initialize O
System O
agent O
S O
; O
2Initialization O
: O
initialize O
the O
simulator O
sampling O
distribution O
pas O
a O
uniform O
distribution O
. O

uniform O
distribution O
, O
similar O
to O
the O
UCB16algorithm O
, O
to O
train O
the O
dialogue O
system O
Sin O
the O
first O
Twarmup B-HyperparameterName
steps O
( O
i.e. O
, O
in O
the O
warm O
- O
up O
phase O
) O
. O

( O
1 O
) O
Warm O
- O
up O
phase O
: O
in O
the O
first O
Twarmup B-HyperparameterName
dialogues O
, O
we O
use O
a O
uniform O
distribution O
to O
sample O
all O
user O
simulators O
to O
train O
the O
system O
agent O
S O
( O
lines O
4 O
- O
7 O
) O
. O

When O
this O
phase O
begins O
( O
i.e. O
, O
t= O
0 O
) O
, O
we O
will O
first O
evaluate O
the O
performance O
( O
i.e. O
, O
the O
success O
rate O
¯xj B-MetricName
, O
j∈ O
{ O
1 O
, O
· O
· O
· O
, O
K O
} O
) O
of O
the O
dialogue O
system O
Strained O
after O
the O
warm O
- O
up O
phase O
. O

Inspired O
by O
UCB1 O
( O
Auer O
et O
al O
. O
, O
2002 O
) O
, O
we O
design O
a O
calibrated O
performance O
expectation O
ˆxjof B-MetricName
the O
system O
agent O
Sinteracting O
with O
each O
user O
simulator O
Ujtaking O
exploration O
into O
consideration O
beyond O
pure O
exploitation O
: O
ˆxj= O
¯xj O
/ O
bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
exploitation+ O
/ O
radicaligg O
2 O
lnt O
Tj O
, O
t O
/ O
bracehtipupleft O
/ O
bracehtipdownright O
/ O
bracehtipdownleft O
/ O
bracehtipupright O
exploration O
, O
j∈ O
{ O
1 O
, O
... O
, O
K O
} O
; O
( O
2 O
) O
where O
¯xjis O
the O
success O
rate O
of O
the O
system O
agent O
S O
tested O
with O
user O
simulator O
Uj O
, O
and O
Tj O
, O
tis O
the O
number O
of O
times O
user O
simulator O
Ujhas O
been O
selected O
with O
so O
far O
. O

Where O
the O
hyperparameter O
τis B-HyperparameterName
the O
smooth B-HyperparameterName
factor I-HyperparameterName
for O
distribution O
p= O
{ O
p1 O
, O
··· O
, O
pK O
} O
– O
the O
larger O
τis O
, O
the O
sharper O
pis O
. O

5 O
Experiments O
To O
verify O
the O
effectiveness O
of O
MUST B-MethodName
, O
we O
benchmark O
the O
system O
agents O
trained O
either O
with O
a O
single O
user O
simulator O
or O
multiple O
user O
simulators O
( O
including O
MUST O
merging O
, O
MUST O
uniform O
, O
and O
MUST O
adaptive O
) O
. O

( O
2019 O
) O
, O
which O
are O
Agenda O
- O
Template O
( O
AgenT B-MethodName
) O
, O
Agenda O
- O
Retrieval O
( O
AgenR B-MethodName
) O
, O
Agenda O
- O
Generation O
( O
AgenG B-MethodName
) O
, O
RNNTemplate O
( O
RNNT B-MethodName
) O
, O
RNN O
- O
Retrieval O
( O
RNNR B-MethodName
) O
, O
RNNEnd2End O
( O
RNN B-MethodName
) O
trained O
with O
different O
dialog O
planning O
and O
generation O
methods O
. O

The O
DM O
modules O
of O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
and O
AgenG B-MethodName
are O
rule O
- O
based O
methods O
. O

The O
DM O
modules O
of O
RNNT B-MethodName
, O
and O
RNNR B-MethodName
are O
using O
Sequicity O
( O
Lei O
et O
al O
. O
, O
2018 O
) O
as O
their O
backbones O
which O
is O
an O
RNN O
- O
based O
seq2seq O
model O
with O
copy O
mechanism O
. O

The O
baselines O
are O
the O
dialogue O
systems O
trained O
by O
each O
user O
simulator O
, O
including O
SysAgenT B-MethodName
, O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
, O
Sys B-MethodName
- I-MethodName
AgenG I-MethodName
, O
Sys B-MethodName
- I-MethodName
RNNT I-MethodName
, O
SysRNNR B-MethodName
, O
and O
Sys B-MethodName
- I-MethodName
RNN I-MethodName
. O

For O
a O
fair O
comparison O
, O
all O
system O
agents O
( O
including O
the O
systems O
trained O
by O
our O
MUST B-MethodName
) O
have O
the O
same O
architecture O
described O
in O
Shi O
et O
al O
. O

MultiWOZ B-DatasetName
Restaurant O
Domain O
Dataset O
. O

The O
original O
task O
in O
MultiWOZ B-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018 O
) O
is O
to O
model O
the O
system O
response O
. O

( O
2019 O
) O
annotate O
the O
user O
intents O
and O
the O
user O
- O
side O
dialog O
acts O
in O
the O
restaurant O
domain O
of O
MultiWOZ B-MethodName
to O
build O
user O
simulators O
, O
which O
has O
a O
total O
of O
1,310 O
dia O
- O
logues O
. O

Moreover O
, O
we O
randomly O
simulate O
2,000 O
dialogues O
from O
each O
rule O
- O
based O
simulator O
( O
i.e. O
, O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
AgenG B-MethodName
) O
and O
their O
corresponding O
system O
agents O
respectively O
, O
and O
processe O
these O
dialogues O
to O
have O
the O
same O
annotation O
format O
as O
the O
MultiWOZ B-DatasetName
restaurant I-DatasetName
domain I-DatasetName
dataset I-DatasetName
. O

We O
denote O
this O
dataset O
asSimulated B-DatasetName
Agenda I-DatasetName
Dataset I-DatasetName
, O
which O
has O
a O
total O
of O
6,000 O
dialogues O
. O

A O
straightforward O
metric O
to O
evaluate O
dialogue O
systems O
is O
the O
success B-MetricName
rate I-MetricName
tested O
by O
each O
user O
simulator O
. O

We O
calculate O
the O
success B-MetricName
rate I-MetricName
between O
a O
user O
simulator O
and O
a O
system O
agent O
by O
sampling O
200 O
dialogues O
. O

We O
exclude O
some O
user O
simulators O
in O
training O
MUST B-MethodName
and O
test O
the O
systems O
with O
them O
as O
out O
- O
of O
- O
domain O
evaluation O
. O

In O
total O
, O
we O
collect O
50 O
dialogues O
for O
each O
dialogue O
system O
to O
calculate O
its O
success B-MetricName
rate I-MetricName
. O

It O
is O
first O
fine O
- O
tuned O
on O
the O
simulated O
agenda O
dataset O
and O
then O
fine O
- O
tuned O
on O
the O
MultiWOZ B-DatasetName
restaurant I-DatasetName
domain I-DatasetName
dataset I-DatasetName
by O
leveraging O
GPT O
. O

This O
user O
simulator O
will O
be O
used O
to O
help O
implementing O
MUST B-MethodName
. O

To O
implement O
the O
MUST B-MethodName
merging I-MethodName
strategy O
, O
similar O
to O
Imitation O
Learning O
( O
IL O
) O
, O
we O
first O
train O
a O
new O
user O
simulator O
with O
dialogue O
sessions O
collected O
from O
different O
user O
simulators O
and O
their O
corresponding O
dialogue O
systems O
. O

GPT O
ILis O
first O
fine O
- O
tuned O
on O
the O
simulated B-DatasetName
agenda I-DatasetName
dataset I-DatasetName
. O

Then O
we O
sample O
1,400 B-HyperparameterValue
dialogues O
from O
the6Dialogue O
SystemsIn O
- O
domain O
evaluation O
Out O
- O
of O
- O
domain O
evaluation O
All O
AgenT B-MethodName
AgenR B-MethodName
RNNT B-MethodName
GPT B-MethodName
AgenG B-MethodName

Each O
column O
represents O
a O
user O
simulator O
, O
each O
row O
represents O
a O
dialogue O
system O
trained O
with O
a O
specific O
simulator O
, O
e.g. O
, O
Sys O
- O
AgenT B-MethodName
means O
the O
system O
trained O
with O
AgenT B-MethodName
. O

Each O
entry O
shows O
the O
success B-MetricName
rate I-MetricName
of O
a O
system O
agent O
when O
dealing O
with O
a O
user O
simulator O
. O

We O
use O
four O
simulators O
( O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
and O
GPT B-MethodName
) O
to O
implement O
MUST B-MethodName
uniform I-MethodName
and O
MUST B-MethodName
adaptive I-MethodName
. O

simulated B-DatasetName
agenda I-DatasetName
dataset I-DatasetName
and O
merge O
them O
with O
1,310 B-HyperparameterValue
MultiWOZ B-DatasetName
restaurant O
domain O
dialogues O
to O
continue O
fine O
- O
tuning O
GPT O
IL O
. O

Sys O
- O
MUST B-MethodName
uniform O
is O
trained O
by O
the O
user O
simulators O
of O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
and O
GPT B-MethodName
with O
a O
uniform O
sampling O
distribution O
. O

For O
training O
SysMUST B-MethodName
adaptive7 O
, O
the O
distribution O
pwill O
be O
adaptively O
updated O
using O
our O
modified O
UCB1 O
algorithm O
. O

We O
also O
train O
the O
Sys O
- O
MUST B-MethodName
uniform I-MethodName
and O
Sys O
- O
MUST B-MethodName
adaptive I-MethodName
by O
using O
different O
subsets O
of O
the O
user O
simulators O
for O
ablation O
studies O
in O
App O
. O

2 O
, O
SysMUST B-MethodName
uniform O
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
outperform O
the O
dialogue O
systems O
( O
Sys B-MethodName
- I-MethodName
AgenT I-MethodName
, O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
, O
SysRNNT B-MethodName
, O
and O
Sys B-MethodName
- I-MethodName
GPT I-MethodName
) O
trained O
by O
a O
single O
user O
simulator O
in O
the O
overall O
performance O
, O
demonstrating O
the O
superiority O
of O
leveraging O
multiple O
user O
simulators O
. O

Especially O
, O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive O
has O
a O
1.2 B-MetricValue
absolute O
value O
improvement O
( O
92.9 B-MetricValue
vs O
. O

91.7 B-MetricValue
) O
averagely O
over O
the O
previous O
SOTA O
system O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
. O

Observing O
that O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
merging I-MethodName
is O
not O
as O
competitive O
as O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
, O
this O
comparison O
shows O
that O
the O
merging O
strategy O
can O
not O
effectively O
leverage O
multiple O
user O
simulators O
. O

Inin O
- O
domain O
evaluation O
, O
the O
performances O
of O
systems O
( O
Sys B-MethodName
- I-MethodName
AgenT I-MethodName
, O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
, O
Sys B-MethodName
- I-MethodName
RNNT I-MethodName
, O
and O
Sys B-MethodName
- I-MethodName
GPT I-MethodName
) O
trained O
by O
a O
single O
user O
simulator O
drop O
a O
lot O
when O
testing O
with O
a O
different O
simulator O
. O

At O
least O
, O
the O
performance O
gap O
of O
dialogue O
systems O
trained O
with O
our O
MUST B-MethodName
becomes O
smaller O
than O
without O
MUST B-MethodName
, O
see O
the O
percentages O
labeled O
in O
green O
and O
red O
colors O
. O

Inout O
- O
of O
- O
domain O
evaluation O
where O
the O
user O
simulators O
used O
for O
testing O
the O
systems O
are O
unseen O
by O
our O
MUST B-MethodName
, O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
SysMUST B-MethodName
adaptive I-MethodName
achieve O
at O
most O
2.4 B-MetricValue
absolute O
value O
improvement O
over O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
. O

This O
evidences O
that O
MUST B-MethodName
has O
a O
better O
generalization O
ability O
for O
interacting O
with O
unseen O
user O
simulators O
. O

Moreover O
, O
the O
dialogue O
systems O
( O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
merging I-MethodName
, O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
, O
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
) O
trained O
with O
the O
proposed O
MUST B-MethodName
approaches O
have O
lower O
standard B-MetricName
deviations I-MetricName
, O
which O
indicates O
that O
they O
are O
more O
robust O
to O
the O
diversity O
of O
user O
simulators O
. O

3 O
, O
the O
human O
evaluation O
results O
show O
that O
our O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
largely O
outperform O
the O
other O
dialogue O
systems O
when O
interacting O
with O
real O
users O
. O

( O
a O
) O
shows O
their O
average O
success B-MetricName
rates I-MetricName
tested O
with O
all O
user O
simulators O
( O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
and O
GPT B-MethodName
) O
. O

The O
success B-MetricName
rates I-MetricName
of O
them O
tested O
with O
each O
user O
simulator O
are O
shown O
in O
( O
b O
) O
- O
( O
e O
) O
. O

5.4 O
Analysis O
and O
Discussions O
Convergences O
of O
MUST B-MethodName
uniform I-MethodName
and O
MUST B-MethodName
adaptive I-MethodName
.In O

2 O
, O
we O
show O
the O
learning O
curves O
of O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
in O
100,000 B-HyperparameterValue
steps O
; O
the O
first O
40,000 B-HyperparameterValue
steps O
are O
in O
the O
warm O
- O
up O
phase O
for O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
. O

2 O
( O
a O
) O
, O
we O
can O
see O
that O
training O
the O
dialogue O
system O
with O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
and O
GPT B-MethodName
by O
MUST B-MethodName
adaptive I-MethodName
converges O
faster O
than O
by O
MUST B-MethodName
uniform I-MethodName
. O

The O
adaptation O
difficulty O
of O
all O
user O
simulators O
could O
be O
ranked O
like O
AgenR B-MethodName
> O
AgenT B-MethodName
> O
GPT B-MethodName
> O
RNNT B-MethodName
according O
to O
Fig O
. O

To O
check O
whether O
MUST B-MethodName
adaptive I-MethodName
tends O
to O
sample O
harder O
- O
to O
- O
adapt O
user O
simulators O
more O
times O
in O
the O
adaptive O
phase O
, O
as O
assumed O
in O
§ O
4.2 O
, O
we O
visualize O
the O
sampling O
proportions O
of O
alluser O
simulators O
in O
Fig O
. O

We O
could O
observe O
that O
AgenR B-MethodName
was O
sampled O
with O
45.1 B-HyperparameterValue
% I-HyperparameterValue
( O
the O
biggest O
proportion O
) O
and O
it O
is O
indeed O
the O
hardest O
user O
simulator O
that O
can O
be O
adapted O
by O
the O
system O
; O
RNNT B-MethodName
has O
the O
smallest O
sampling O
proportion O
and O
it O
is O
the O
easiest O
user O
simulator O
that O
can O
be O
adapted O
by O
the O
system O
. O

Interestingly O
, O
it O
shows O
that O
AgenR B-MethodName
and O
AgenT B-MethodName
are O
competitive O
with O
the O
GPT B-MethodName
simulator O
; O
while O
RNNT B-MethodName
and O
GPT B-MethodName
are O
cooperative O
with O
each O
other O
. O

This O
might O
be O
because O
both O
RNNT B-MethodName
and O
GPT B-MethodName
simulators O
are O
learned O
from O
the O
dialogue O
corpus O
and O
might O
share O
some O
similar O
behaviors O
. O

6 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
a O
framework O
named O
MUST B-MethodName
to O
improve O
dialogue O
systems O
by O
using O
multiple O
user O
simulators O
simultaneously O
. O

We O
discuss O
several O
simple O
methods O
to O
implement O
MUST B-MethodName
, O
which O
is O
either O
inflexible O
or O
inefficient O
. O

Therefore O
, O
we O
formulate O
MUST B-MethodName
as O
a O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandits I-TaskName
( I-TaskName
MAB I-TaskName
) I-TaskName
problem O
, O
based O
on O
which O
we O
propose O
a O
novel O
implementation O
called O
MUST B-MethodName
adaptive I-MethodName
. O

The O
experimental O
results O
on O
the O
restaurant O
search O
task O
from8MultiWOZ B-DatasetName
demonstrate O
that O
MUST B-MethodName
can O
largely O
improve O
the O
system O
agent O
upon O
baselines O
, O
especially O
when O
tested O
with O
unseen O
user O
simulators O
. O

Moreover O
, O
MUST B-MethodName
adaptive I-MethodName
is O
more O
efficient O
than O
other O
implementations O
. O

Limitation O
The O
main O
limitation O
of O
this O
work O
is O
that O
we O
only O
conduct O
our O
experiments O
on O
the O
restaurant O
domain O
of O
the O
MultiWOZ B-DatasetName
since O
we O
can O
only O
find O
multiple O
user O
simulators O
from O
Shi O
et O
al O
. O

A O
Multi B-TaskName
- I-TaskName
armed I-TaskName
bandit I-TaskName
problem I-TaskName
Reinforcement O
learning O
policies O
face O
the O
exploitation O
versus O
exploration O
trade O
- O
off O
, O
which O
can O
be O
described O
as O
the O
search O
for O
a O
balance O
between O
exploring O
the O
environment O
to O
find O
profitable O
actions O
while O
taking O
the O
empirically O
best O
action O
as O
often O
as O
possible O
. O

At O
each O
time O
step O
t= O
1,2 O
, O
... O
, O
T O
, O
the O
decision O
maker O
must O
choose O
one O
of O
these O
Karms B-HyperparameterName
. O

4.B.2 O
The O
implementations O
of O
the O
dialogue O
systems O
The O
NLU O
modules O
of O
all O
system O
agents O
are O
a O
2 O
- O
layer O
bidirectional O
- O
GRU O
with O
200 B-HyperparameterValue
hidden O
units O
. O

The O
reward B-HyperparameterName
will O
be O
given O
+1 B-HyperparameterValue
for O
task O
success O
, O
-1 B-HyperparameterValue
for O
task O
failure O
, O
and O
-0.1 B-HyperparameterValue
for O
each O
additional O
turn O
to O
encourage O
the O
RL O
- O
based O
policy O
module O
to O
finish O
the O
task O
fast O
. O

Also O
, O
a O
discounted B-HyperparameterName
factor I-HyperparameterName
of O
0.9 B-HyperparameterValue
is O
applied O
to O
all O
the O
experiences O
. O

The O
parameters O
of O
training O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName

The O
hyperparameters O
used O
to O
train O
the O
SysMUST B-MethodName
adaptive I-MethodName
are O
listed O
in O
the O
Tab O
. O

Since O
some O
user O
simulators O
used O
for O
implementing O
our O
MUST B-MethodName
framework O
are O
based O
on O
the O
GPT O
model O
, O
we O
train O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
on O
a O
V100 O
GPU O

Then O
we O
prepare O
50 B-HyperparameterValue
user O
goals O
from O
MultiWOZ B-DatasetName
Restaurant I-DatasetName
Domain I-DatasetName
Dataset I-DatasetName
: O
20 B-HyperparameterValue
of O
them O
are O
simple O
, O
and O
30 B-HyperparameterValue
of O
them O
are O
a O
little O
bit O
complex O
. O

We O
specify O
10 B-HyperparameterValue
user O
goals O
for O
each O
volunteer O
and O
let O
the O
volunteer O
converse O
with O
all O
dialogue O
systems O
for O
each O
same O
user O
goal O
. O

Hyperparameter O
Value O
T B-HyperparameterName
100,000 B-HyperparameterValue
T0 B-HyperparameterName
40,000 B-HyperparameterValue
e B-HyperparameterName
2,000 B-HyperparameterValue
d B-HyperparameterName
200 B-HyperparameterValue
τ B-HyperparameterName
0.75 B-HyperparameterValue

Table O
5 O
: O
The O
hyperparameters O
used O
for O
training O
the O
SysMUST B-MethodName
adaptive I-MethodName
. O

dialogue O
system O
to O
calculate O
its O
success B-MetricName
rate I-MetricName
. O

C O
Implement O
MUST O
with O
the O
MUST O
CRL O
strategy O
Without O
losing O
any O
generality O
, O
we O
consider O
two O
representative O
sequential O
orders O
: O
1 O
) O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
GPT B-MethodName
; O
and O
2 O
) O
AgenR B-MethodName
, O
GPT B-MethodName
, O
AgenT B-MethodName
, O
RNNT B-MethodName
. O

7 O
, O
in O
case O
1 O
, O
the O
system O
agent O
achieves O
the O
best O
performance O
( O
i.e. O
, O
92.4 B-MetricValue
in O
terms O
of O
the O
average B-MetricName
success I-MetricName
rate I-MetricName
) O
after O
training O
with O
AgenT B-MethodName
and O
AgenR B-MethodName
sequentially O
. O

However O
, O
its O
overall O
performance O
degrades O
to O
83.0 B-MetricValue
after O
training O
with O
RNNT B-MethodName
; O
especially O
, O
its O
performance O
decreases O
by O
36.0 O
% O
when O
testing O
with O
AgenR B-MethodName
( O
93.0→59.5 B-MetricValue
) O
. O

Moreover O
, O
after O
continuing O
to O
learn O
from O
GPT B-MethodName
, O
the O
performance O
of O
the O
system O
agent O
becomes O
worse O
for O
AgenT B-MethodName
( O
95.0→75.5 B-MetricValue
) O
and O
AgenR B-MethodName
( O
59.5→ O
47.5 B-MetricValue
) O
. O

This O
indicates O
the O
catastrophic O
forgetting O
issue O
heavily O
happened O
when O
the O
system O
agent O
starts O
learning O
from O
AgenR B-MethodName
. O

These O
results O
can O
confirm O
that O
implementing O
our O
proposed O
MUST B-MethodName
with O
MUST O
CRL O
strategy O
indeed O
has O
the O
catastrophic O
forgetting O
issue O
. O

D O
Sensitivity O
on O
different O
subsets O
of O
user O
simulators O
We O
also O
train O
the O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
SysMUST B-MethodName
adaptive I-MethodName
by O
using O
different O
groups O
of O
user O
simulators O
for O
ablation O
studies O
: O
1 O
) O
five O
user O
simulators O
of O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
RNNR B-MethodName
, O
and O
GPT B-MethodName
; O
and O
2 O
) O
three O
user O
simulators O
including O
AgenT B-MethodName
, O
RNNT B-MethodName
, O
and O
GPT B-MethodName
. O

SysMUST B-MethodName
adaptive I-MethodName
largely O
outperform O
the O
dialogue O
systems O
trained O
by O
single O
user O
simulators O
. O

8 O
, O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
achieves O
a O
2.7 O
absolute O
value O
improvement O
( O
92.5 B-MetricValue
vs O
89.8 B-MetricValue
) O
over O
Sys B-MethodName
- I-MethodName
AgenR I-MethodName
. O

Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
even O
improve O
at O
least O
5.7 B-MetricValue
points O
( O
80.0 B-MetricValue
vs O
74.3 B-MetricValue
) O
over O
Sys B-MethodName
- I-MethodName
GPT I-MethodName
( O
as O
shown O
in O
Tab O
. O

These O
experimental O
results O
on O
different O
subsets O
of O
user O
simulators O
demonstrate O
that O
our O
MUST B-MethodName
has O
a O
better O
generalization O
ability O
for O
interacting O
with O
unseen O
user O
simulators O
and O
is O
insensitive O
to O
the O
user O
simulator O
selection O
. O

Comparison O
between O
MUST B-MethodName
uniform I-MethodName
and O
MUST B-MethodName
adaptive I-MethodName
.Fig O
. O

4 O
shows O
the O
learning O
curves O
of O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
on O
different O
subsets O
of O
user O
simulators O
. O

The O
first O
40,000 B-HyperparameterValue
steps O
are O
in O
the O
warm O
- O
up O
phase O
for O
SysMUST B-MethodName
adaptive I-MethodName
. O

We O
could O
conclude O
that O
training O
the O
dialogue O
system O
by O
MUST B-MethodName
adaptive I-MethodName
consistently O
converges O
faster O
than O
by O
MUST B-MethodName
uniform I-MethodName
, O
at O
least O
in O
the O
scenarios O
when O
using O
three O
, O
four O
, O
or O
five O
user O
simulators O
to O
implement O
MUST B-MethodName
( O
see O
Fig O
. O

8 O
where O
MUST B-MethodName
is O
trained O
with O
five O
user O
simulators O
, O
we O
could O
observe O
that O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
outperforms O
SysMUST B-MethodName
uniform I-MethodName
with O
0.5 B-MetricValue
absolute O
point O
. O

The O
performance O
gain O
becomes O
smaller O
when O
MUST B-MethodName
is O
trained O
with O
three O
user O
simulators O
( O
see O
Tab O
. O

This O
probably O
shows O
that O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
would O
be O
more O
beneficial O
when O
there O
exist O
more O
user O
simulators O
. O

F O
Implementing O
MUST B-MethodName
with O
more O
user O
simulators O
To O
implement O
our O
MUST B-MethodName
with O
more O
user O
simulators O
, O
we O
use O
Simulated B-DatasetName
Agenda I-DatasetName
Dataset I-DatasetName
to O
train O
four O
extra O
user O
simulators8 O
. O

6 O
( O
a O
) O
shows O
the O
learning O
curve O
of O
the O
system O
agent O
trained O
by O
MUST B-MethodName
with O
eight O
simulators O
( O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
RNNT B-MethodName
, O
GPT B-MethodName
, O
GPT B-MethodName
AT I-MethodName
, O
GPT B-MethodName
AR I-MethodName
, O
GPT B-MethodName
AG I-MethodName
, O
and O
GPT B-MethodName
rand I-MethodName
) O
. O

We O
could O
observe O
that O
the O
training O
of O
our O
proposed O
MUST B-MethodName
can O
still O
succeed O
when O
we O
increase O
the O
number O
of O
user O
simulators O
to O
eight O
. O

Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
still O
converges O
faster O
than O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
even O
though O
the O
difference O
between O
their O
convergence O
speeds O
is O
not O
too O
large O
in O
this O
case O
. O

It O
might O
be O
because O
some O
user O
simulators O
are O
similar O
( O
e.g. O
, O
GPT B-MethodName
ATis I-MethodName
similar O
to O
AgenT B-MethodName
, O
GPT B-MethodName
ARis I-MethodName
similar O
to O
AgenR B-MethodName
) O
, O
which O
might O
lead O
that O
the O
distribution O
p O
approaches O
a O
uniform O
distribution O
. O

6 O
( O
b O
) O
compares O
the O
learning O
curves O
of O
SysMUST B-MethodName
adaptive I-MethodName
and O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
trained O
with O
different O
numbers O
of O
user O
simulators O
( O
i.e. O
, O
four O
, O
five O
, O
and O
eight O
user O
simulators O
) O
. O

It O
is O
a O
fair O
comparison O
because O
these O
combinations O
include O
the O
hardest O
8Simulated B-DatasetName
Agenda I-DatasetName
Dataset I-DatasetName
( O
See O
Sec O
. O

5.1 O
) O
is O
simulated O
from O
each O
rule O
- O
based O
user O
simulator O
( O
i.e. O
, O
AgenT B-MethodName
, O
AgenR B-MethodName
, O
AgenG B-MethodName
) O
and O
its O
corresponding O
system O
agent O
respectively O
. O

we O
also O
collect O
3000 B-HyperparameterValue
dialogues O
randomly O
from O
Simulated B-DatasetName
Agenda I-DatasetName
Dataset I-DatasetName
to O
train O
another O
new O
GPT O
user O
simulator O
denoted O
as O
GPT O
rand.user O
simulator O
AgenR B-MethodName
that O
can O
be O
adapted O
by O
the O
system O
and O
the O
easiest O
user O
simulator O
RNNT B-MethodName
that O
can O
be O
adapted O
by O
the O
system O
( O
See O
Sec O
. O

We O
can O
observe O
that O
, O
with O
more O
user O
simulators O
, O
SysMUST B-MethodName
adaptive I-MethodName
not O
only O
performs O
better O
but O
also O
converges O
faster O
than O
with O
fewer O
user O
simulators O
. O

This O
probably O
shows O
that O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
adaptive I-MethodName
has O
the O
potential O
to O
be O
generalized O
to O
a O
larger O
set O
of O
user O
simulators O
. O

Plus O
, O
we O
also O
could O
observe O
that O
SysMUST B-MethodName
adaptive I-MethodName
consistently O
converges O
faster O
than O
Sys B-MethodName
- I-MethodName
MUST I-MethodName
uniform I-MethodName
in O
different O
numbers O
of O
user O
simulators O
. O

G O
Modeling O
User O
Simulator O
with O
GPT B-MethodName
We O
name O
the O
model O
of O
building O
a O
user O
simulator O
based O
on O
GPT B-MethodName
as O
U B-MethodName
- I-MethodName
GPT I-MethodName
. O

The O
architecture O
of O
U B-MethodName
- I-MethodName
GPT I-MethodName
As O
Fig O
. O

7 O
( O
a O
) O
shown O
, O
our O
U B-MethodName
- I-MethodName
GPT I-MethodName
consists O
of O
four O
modules O
, O
which O
are O
Natural O
Language O
Understanding O
( O
NLU O
) O
, O
Goal O
Generator O
, O
Dialog O
Policy O
Learning O
( O
POL O
) O
, O
and O
Natural O
Language O
Generation O
( O
NLG O
) O
. O

In O
the O
first O
turn O
t= O
0 O
, O
U B-MethodName
- I-MethodName
GPT I-MethodName
( O
1 O
) O
first O
outputs O
its O
NLU O
results O

For O
training O
U B-MethodName
- I-MethodName
GPT I-MethodName
, O
we O
use O
the O
same O
training O
objective O
as O
GPT B-MethodName
which O
is O
to O
maximize O
the O
following O
likelihood O
: O
L O
( O
U O
) O
= O
/ O
summationdisplay O
ilogP O
( O
ui|ui−k O
, O
... O
, O
u O
i−1 O
; O
Θ O
) O
, O
∀ui∈ O
{ O
S0 O
, O
N0 O
, O
G0 O
, O
A0 O
, O
U0 O
, O
... O
, O
A O
t O
, O
Ut O
} O
, O
where O
kis B-HyperparameterName
the O
size B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
context I-HyperparameterName
window I-HyperparameterName
, O
and O
the O
conditional O
probability O
Pis O
parameterized O
with O
Θ O
. O

G.2 O
Evaluations O
on O
U B-MethodName
- I-MethodName
GPT I-MethodName
To O
evaluate O
our O
proposed O
U B-MethodName
- I-MethodName
GPT I-MethodName
, O
we O
adopt O
both O
indirect O
evaluations O
and O
direct O
evaluations O
as O
in O
Shi O
et O
al O
. O

We O
evaluate O
a O
user O
simulator O
indirectly O
using O
the O
average B-MetricName
success I-MetricName
rate I-MetricName
of O
the O
system O
agent O
trained O
by O
this O
simulator O
. O

For O
direct O
evaluations O
, O
we O
adopt O
six O
evaluation O
measures O
to O
evaluate O
the O
diversity O
of O
user O
simulators O
automatically O
: O
average B-MetricName
utterance I-MetricName
length I-MetricName
, O
vocabulary B-MetricName
size I-MetricName
, O
Dist-1 B-MetricName
, O
Dist-2 B-MetricName
( O
Li O
et O
al O
. O
, O
2016a O
) O
and O
Entropy B-MetricName
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O

( O
2019 O
) O
which O
are O
Fluency B-MetricName
, O
Coherence B-MetricName
, O
Goal B-MetricName
Adherence I-MetricName
, O
Diversity B-MetricName
, O
and O
Overall B-MetricName
quality I-MetricName
to O
assess O
user O
simulators O
from O
multiple O
aspects O
. O

G.3 O
Training O
details O
of O
user O
simulators O
We O
implement O
our O
GPT B-MethodName
- O
based O
user O
simulators O
with O
DistilGPT2 O
( O
Sanh O
et O
al O
. O
, O
2020 O
) O
, O
a O
distilled O
version O
of O
GPT-2 O
by O
HuggingFace O
’s O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
. O

search O
of O
learning B-HyperparameterName
rate I-HyperparameterName
and O
batch B-HyperparameterName
size I-HyperparameterName
. O

The O
best O
models O
were O
fine O
- O
tuned O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-3 B-HyperparameterValue
over O
the O
corresponding O
dataset O
. O

Because O
the O
implementation O
of O
user O
simulator O
RNN O
mainly O
consists O
of O
NLU O
and O
NLG O
, O
we O
remove O
the O
POL O
module O
from O
U B-MethodName
- I-MethodName
GPT I-MethodName
and O
use O
the O
same O
annotated O
data O
as O
RNN O
to O
fine O
- O
tune O
it O
to O
compare O
our O
U B-MethodName
- I-MethodName
GPT I-MethodName
with O
the O
RNN O
- O
based O
methods O
fairly O
and O
name O
it O
as O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
. O

13 O
show O
, O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
outperforms O
the O
user O
simulator O
RNN O
. O

It O
proves O
the O
power O
of O
leveraging O
GPT B-MethodName
. O

Our O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
performs O
better O
than O
the O
user O
simulator O
RNNT B-MethodName
, O
which O
can O
be O
seen O
from O
the O
crossmodel O
evaluation O
results O
in O
Tab O
. O

13 O
shows O
, O
RNNT B-MethodName
performs O
better O
than O
our O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
in O
the O
overall O
performance O
from O
the O
human O
evaluation O
. O

We O
think O
this O
might O
be O
because O
( O
1 O
) O
the O
third O
- O
party O
system O
also O
has O
an O
impact O
on O
the O
generated O
dialogues O
and O
( O
2 O
) O
the O
NLG O
module O
of O
RNNT B-MethodName
is O
the O
template O
- O
based O
method O
which O
leads O
tothe O
generated O
dialogues O
from O
RNNT B-MethodName
being O
easy O
for O
the O
third O
- O
party O
system O
to O
understand O
and O
interact O
with O
. O

We O
think O
it O
is O
because O
the O
user O
utterances O
generated O
by O
RNNR B-MethodName
are O
retrieved O
from O
a O
corpus O
that O
is O
written O
by O
real O
humans O
and O
the O
sentences O
written O
by O
humans O
are O
usually O
more O
diverse O
than O
the O
sentences O
generated O
by O
generative O
models O
. O

Even O
though O
the O
dialogues O
generated O
by O
RNNR B-MethodName
are O
more O
diverse O
, O
the O
dialogues O
generated O
by O
our O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
are O
more O
fluent O
and O
coherent O
. O

11 O
show O
that O
GPT B-MethodName
- I-MethodName
RNN I-MethodName
can O
help O
to O
learn O
a O
more O
robust O
system O
agent O
than O
RNNR B-MethodName
, O
but O
the O
Hu O
. O


Proceedings O
of O
the O
61st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
364–376 O
July O
9 O
- O
14 O
, O
2023 O
© O
2023 O
Association O
for O
Computational O
Linguistics O
Rule O
By O
Example O
: O
Harnessing O
Logical O
Rules O
for O
Explainable O
Hate B-TaskName
Speech I-TaskName
Detection I-TaskName
Christopher O
Clarke˚ O
: O
Matthew O
Hall O
; O
Gaurav O
Mittal O
; O
Ye O
Yu O
; O
Sandra O
Sajeev O
; O
Jason O
Mars O
: O
Mei O
Chen O
; O
: O
University O
of O
Michigan O
, O
Ann O
Arbor O
, O
MI O
; O
Microsoft O
, O
Redmond O
, O
WA O
{ O
csclarke O
, O
profmars O
} O
@ O
umich.edu O
{ O
mathall O
, O
gaurav.mittal O
, O
yu.ye O
, O
ssajeev O
, O
mei.chen O
} O
@ O
microsoft.com O
Abstract O
Classic O
approaches O
to O
content O
moderation O
typically O
apply O
a O
rule O
- O
based O
heuristic O
approach O
to O
flag O
content O
. O

In O
this O
paper O
, O
we O
present O
Rule B-MethodName
By I-MethodName
Example I-MethodName
( O
RBE B-MethodName
) O
: O
a O
novel O
exemplarbased O
contrastive O
learning O
approach O
for O
learning O
from O
logical O
rules O
for O
the O
task O
of O
textual O
content O
moderation O
. O

RBE B-MethodName
is O
capable O
of O
providing O
rule O
- O
grounded O
predictions O
, O
allowing O
for O
more O
explainable O
and O
customizable O
predictions O
compared O
to O
typical O
deep O
learning O
- O
based O
approaches O
. O

Experimental O
results O
on O
3 O
popular O
hate O
speech O
classification O
datasets O
show O
that O
RBE B-MethodName
is O
able O
to O
outperform O
state O
- O
of O
- O
the O
- O
art O
deep O
learning O
classifiers O
as O
well O
as O
the O
use O
of O
rules O
in O
both O
supervised O
and O
unsupervised O
settings O
while O
providing O
explainable O
model O
predictions O
via O
rulegrounding O
. O

In O
contrast O
to O
the O
challenges O
faced O
by O
rule B-MethodName
- I-MethodName
based I-MethodName
methods I-MethodName
, O
data O
- O
driven O
deep O
learning O
approaches O
have O
shown O
great O
promise O
across O
a O
wide O
range O
of O
content O
moderation O
tasks O
and O
modalities O
( O
Malik O
et O
al O
. O
, O
2022 O
; O
Shido O
et O
al O
. O
, O
2022 O
; O
Lai O
et O
al O
. O
, O
2022 O
) O
. O

In O
order O
to O
tackle O
this O
problem O
, O
we O
introduce O
Rule B-MethodName
By I-MethodName
Example I-MethodName
( O
RBE B-MethodName
) O
: O
a O
novel O
exemplar O
- O
based O
1https O
: O
/ O
/ O
perspectiveapi.com O
/ O
2https O
: O
/ O
/ O
openai.com O
/ O
blog O
/ O
new O
- O
and O
- O
improved O
- O
content O
- O
moderation O
% O
2Dtooling O
/ O
3https O
: O
/ O
/ O
azure.microsoft.com O
/ O
en O
- O
us O
/ O
products O
/ O
cognitive O
- O
services O
/ O
content O
- O
moderator O
/ O
contrastive O
learning O
approach O
for O
learning O
from O
logical O
rules O
for O
the O
task O
of O
textual B-TaskName
content I-TaskName
moderation I-TaskName
. O

RBE B-MethodName
is O
comprised O
of O
two O
neural O
networks O
, O
a O
rule O
encoder O
, O
and O
a O
text O
encoder O
, O
which O
jointly O
learn O
rich O
embedding O
representations O
for O
hateful O
content O
and O
the O
logical O
rules O
that O
govern O
them O
. O

Through O
the O
use O
of O
contrastive O
learning O
, O
our O
framework O
uses O
a O
semantic B-MetricName
similarity I-MetricName
objective O
that O
pairs O
hateful O
examples O
with O
clusters O
of O
rule O
exemplars O
that O
govern O
it O
. O

Through O
this O
approach O
, O
RBE B-MethodName
is O
able O
to O
provide O
more O
explainable O
predictions O
by O
allowing O
for O
what O
we O
define O
as O
Rule O
- O
grounding O
. O

We O
evaluate O
RBE B-MethodName
in O
both O
supervised O
and O
unsupervised O
settings O
using O
a O
suite O
of O
rulesets O
. O

Our O
results O
show O
that O
with O
as O
little O
as O
one O
exemplar O
per O
rule O
, O
RBE B-MethodName
is O
capable O
of O
outperforming O
state O
- O
of O
- O
theart O
hateful O
text O
classifiers O
across O
three O
benchmark O
content O
moderation O
datasets O
in O
both O
settings O
. O

In O
summary O
, O
the O
contributions O
of O
this O
paper O
are O
: O
•Rule B-MethodName
By I-MethodName
Example I-MethodName
( O
RBE B-MethodName
) O
: O
a O
novel O
exemplarbased O
contrastive O
learning O
approach O
to O
learn O
from O
logical O
rules O
for O
the O
task O
of O
textual O
content O
moderation.4 O
•We O
demonstrate O
how O
RBE B-MethodName
can O
be O
easily O
integrated O
to O
boost O
model O
F1 B-MetricName
- O
score O
by O
up O
to O
4 B-MetricValue
% I-MetricValue
on O
three O
popular O
hate O
speech O
classification O
datasets O
. O

•A O
detailed O
analysis O
and O
insights O
into O
the O
customizability O
and O
interpretability O
features O
of O
RBE B-MethodName
to O
address O
the O
problem O
of O
emerging O
hateful O
content O
and O
model O
transparency O
. O

2 O
Rule O
By O
Example O
Framework O
In O
this O
section O
, O
we O
outline O
the O
Rule B-MethodName
By I-MethodName
Example I-MethodName
framework O
, O
define O
its O
operational O
terms O
, O
and O
describe O
its O
end O
- O
to O
- O
end O
architecture O
. O

Figure O
1 O
shows O
an O
example O
of O
a O
simple O
rule O
that O
is O
triggered O
if O
a O
given O
text O
contains O
the O
keywords O
“ O
hate O
” O
or O
4https O
: O
/ O
/ O
github.com O
/ O
ChrisIsKing O
/ O
Rule O
- O
By O
- O
Example365Figure O
2 O
: O
Rule B-MethodName
By I-MethodName
Example I-MethodName
Framework O
: O
RBE B-MethodName
is O
comprised O
of O
two O
neural O
networks O
, O
a O
rule O
encoder O
and O
a O
text O
encoder O
, O
which O
jointly O
learn O
rich O
embedding O
representations O
for O
hateful O
content O
and O
the O
logical O
rules O
that O
govern O
them O
. O

Through O
Contrastive O
learning O
, O
RBE B-MethodName
utilizes O
a O
semantic B-MetricName
similarity I-MetricName
objective O
that O
pairs O
hateful O
examples O
with O
clusters O
of O
rule O
exemplars O
that O
govern O
it O
. O

Contrastive O
learning O
encourages O
the O
model O
to O
maximize O
the O
representation O
similarity B-MetricName
between O
same O
- O
label O
examples O
and O
to O
minimize O
it O
for O
different O
- O
label O
examples O
. O

2.2 O
Rule O
- O
Grounding O
By O
taking O
an O
embeddings O
- O
based O
approach O
to O
learning O
representations O
, O
RBE B-MethodName
enables O
what O
we O
define O
asrule O
- O
grounding O
. O

Table O
2 O
shows O
an O
example O
of O
this.3 O
Experimental O
Setup O
Training O
We O
train O
all O
models O
with O
AdamW O
optimizer B-HyperparameterName
and O
weight B-HyperparameterName
decay I-HyperparameterName
of O
0.01 B-HyperparameterValue
on O
all O
data O
. O

We O
employ O
early B-HyperparameterName
stopping I-HyperparameterName
with O
a O
ceiling O
of O
10 B-HyperparameterValue
epochs O
, O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
, O
and O
linear O
learning O
rate O
warmup O
over O
the O
first O
10 O
% O
steps O
with O
a O
cosine O
schedule O
. O

Our O
implementation O
of O
RBE B-MethodName
is O
based O
on O
Huggingface O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
and O
Sentence O
Transformers O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O

RBE B-MethodName
utilizes O
two O
Bert O
- O
based O
networks O
consisting O
of O
110 O
million O
parameters O
each O
. O

Approximately O
2,000 O
GPU O
hours O
were O
required O
to O
train O
all O
hyperparameter O
variations O
of O
RBE B-MethodName
plus O
the O
Bert O
baseline O
across O
all O
3 O
test O
sets O
. O

3.1 O
Datasets O
We O
evaluate O
RBE B-MethodName
across O
three O
datasets O
on O
the O
task O
of O
hate B-TaskName
- I-TaskName
speech I-TaskName
classification I-TaskName
. O

HateXplain B-DatasetName
( O
Mathew O
et O
al O
. O
, O
2020 O
) O
is O
a O
large O
- O
scale O
benchmark O
dataset O
for O
explainable O
hate B-TaskName
speech I-TaskName
detection I-TaskName
that O
covers O
multiple O
aspects O
of O
hate B-TaskName
speech I-TaskName
detection I-TaskName
. O

Additionally O
, O
we O
utilize O
the O
accompanying O
rationales O
for O
ruleset O
construction.367Jigsaw5is B-DatasetName
a O
large O
- O
scale O
dataset O
of O
Wikipedia O
comments O
labeled O
by O
human O
raters O
for O
toxic O
behavior O
. O

Contextual B-DatasetName
Abuse I-DatasetName
Dataset I-DatasetName
( O
CAD B-DatasetName
) O
( O
Vidgen O
et O
al O
. O
, O
2021 O
) O
is O
annotated O
dataset O
of O
„ O
25k O
Reddit O
entries O
labeled O
across O
six O
conceptually O
distinct O
primary O
categories O
of O
“ O
Identity O
- O
directed O
” O
, O
“ O
Persondirected O
” O
, O
“ O
Affiliation O
directed O
” O
, O
“ O
Counter O
Speech O
” O
, O
“ O
Non O
- O
hateful O
Slurs O
” O
, O
and O
“ O
Neutral O
” O
. O

In O
total O
, O
Hate+Abuse O
List O
consists O
of O
2957 B-HyperparameterValue
distinct O
identity O
hate O
rules O
. O

HateXplain O
Rationale O
Ruleset O
Using O
the O
labeled O
annotator O
rationales O
included O
in O
the O
HateXplain B-DatasetName
dataset O
, O
we O
programmatically O
generate O
a O
Ruleset O
for O
HateXplain B-DatasetName
. O

Using O
a O
default O
cluster B-HyperparameterName
size I-HyperparameterName
of O
100 B-HyperparameterValue
across O
the O
25 O
target O
categories O
defined O
in O
HateXplain O
, O
we O
generated O
a O
total O
of O
670 B-HyperparameterValue
distinct O
rules O
for O
HateXplain B-DatasetName
. O

5https O
: O
/ O
/ O
www.kaggle.com O
/ O
competitions O
/ O
jigsaw O
- O
toxic O
- O
comment O
- O
classification O
% O
2DchallengeContextual O
Abuse O
Rationale O
Ruleset O
Similar O
to O
our O
derived O
HateXplain O
ruleset O
we O
programmatically O
generate O
a O
Ruleset O
for O
the O
Contextual B-DatasetName
Abuse I-DatasetName
Dataset I-DatasetName
using O
annotator O
- O
labeled O
rationales O
. O

Following O
the O
identical O
process O
outlined O
before O
, O
this O
results O
in O
a O
total O
of O
2712 B-HyperparameterValue
distinct O
rules O
for O
CAD B-DatasetName
. O

3.3 O
Unsupervised O
Setting O
In O
addition O
to O
evaluating O
RBE B-MethodName
in O
supervised O
settings O
, O
we O
investigate O
the O
applicability O
of O
RBE B-MethodName
in O
unsupervised O
settings O
where O
no O
labeled O
data O
is O
present O
. O

avgDistpEΘpciqq“1 O
nnÿ O
idistpci O
j O
, O
ci O
j´1q O
( O
4 O
) O
Lastly O
, O
once O
the O
average O
distance O
for O
each O
rule O
is O
calculated O
, O
using O
the O
defined O
threshold O
k O
, O
we O
flip O
any O
weakly O
labeled O
examples O
in O
the O
cover O
set O
if O
the O
average O
distance O
for O
that O
rule O
is O
above O
the O
threshold O
k O
: O
fptiq O
“ O
# O
1 O
, O
ifavgDistpriqěk O
0 O
, O
otherwise O
( O
5 O
) O
4 O
Results O
and O
Discussion O
We O
analyze O
the O
results O
of O
our O
experiments O
, O
detail O
our O
insights O
, O
and O
discuss O
the O
implications O
of O
applying O
RBE B-MethodName
for O
explainable O
hate B-TaskName
speech I-TaskName
detection I-TaskName
.Evaluation O
Metrics O
: O
The O
precision B-MetricName
, O
recall B-MetricName
, O
and O
F1 B-MetricName
score O
for O
each O
dataset O
in O
a O
supervised O
setting O
are O
reported O
in O
Table O
1 O
. O

Due O
to O
the O
highly O
skewed O
class O
distribution O
, O
we O
favor O
macro B-MetricName
F1 I-MetricName
scores O
as O
our O
main O
evaluation O
metric O
. O

We O
also O
report O
accuracy B-MetricName
scores O
( O
the O
fraction O
of O
entries O
for O
which O
the O
full O
set O
of O
labels O
matches O
) O
as O
another O
metric O
. O

4.1 O
Supervised O
Performance O
Table O
1 O
reports O
our O
results O
on O
three O
hate B-TaskName
speech I-TaskName
classification I-TaskName
datasets O
in O
the O
supervised O
setting O
. O

We O
observe O
that O
RBE B-MethodName
is O
able O
to O
outperform O
SOTA O
transformer O
- O
based O
models O
BERT O
and O
MPNet O
by O
1.3 O
/ O
1.4 B-MetricValue
% I-MetricValue
, O
4.1 O
/ O
2.3 B-MetricValue
% I-MetricValue
, O
and O
4.3 O
/ O
1.3 B-MetricValue
% I-MetricValue
in O
F1 B-MetricName
- O
score O
on O
HateXplain B-DatasetName
, O
Jigsaw B-DatasetName
, O
and O
CAD B-DatasetName
respectively O
. O

This O
further O
showcases O
how O
lightweight O
and O
flexible O
RBE B-MethodName
is O
to O
integrate O
into O
a O
content O
moderation O
workflow O
. O

For O
HateXplain B-DatasetName
, O
our O
experiments O
show O
that O
the O
combination O
of O
MPNet O
as O
the O
initialized O
encoder O
with O
both O
the O
HateXplain B-DatasetName
Rationale O
and O
Hate+Abuse O
Ruleset O
delivers O
the O
best O
performance O
. O

by O
the O
high O
recall B-MetricName
score O
of O
the O
HateXplain O
Rationale O
Ruleset O
in O
Table O
1 O
. O

Additionally O
, O
when O
applied O
to O
the O
HateXplain B-DatasetName
dataset O
, O
the O
HateXplain B-DatasetName
Rationale O
Ruleset O
produces O
a O
total O
of O
577 B-HyperparameterValue
rules O
compared O
to O
the O
377 B-HyperparameterValue
rules O
derived O
from O
the O
Hate+Abuse O
Ruleset O
, O
allowing O
for O
more O
rule O
representations O
for O
the O
model O
to O
contrast O
against O
. O

In O
practice O
, O
the O
BERT B-MethodName
[ O
CLS O
] O
token O
as O
well O
as O
averaged O
BERT B-MethodName
outputs O
can O
contain O
useful O
information O
after O
downstream O
fine O
- O
tuning O
. O

This O
is O
shown O
by O
the O
BERT B-MethodName
performance O
in O
Table O
1 O
. O

However O
, O
when O
the O
pretrained O
model O
output O
is O
pooled O
across O
all O
dimensions O
and O
used O
for O
calculating O
semantic B-MetricName
similarity I-MetricName
, O
this O
results O
in O
similar O
representations O
even O
for O
completely O
different O
input O
text O
. O

As O
a O
result O
, O
if O
applied O
to O
the O
HateXplain B-DatasetName
dataset O
without O
any O
fine O
- O
tuning O
, O
BERT B-MethodName
embeddings O
obtain O
a O
precision B-MetricName
, O
recall B-MetricName
, O
and O
F1 B-MetricName
- O
score O
of O
59 B-MetricValue
% I-MetricValue
, O
100 B-MetricValue
% I-MetricValue
, O
and O
75 B-MetricValue
% I-MetricValue
respectively O
, O
where O
every O
example O
is O
labeled O
as O
hateful O
. O

This O
lack O
of O
varied O
sentence O
representation O
coupled O
with O
a O
verbose O
ruleset O
such O
as O
the O
HateXplain B-DatasetName
Rationale O
Ruleset O
results O
in O
an O
initial O
biasing O
towards O
hateful O
examples O
as O
shown O
by O
the O
high O
recall B-MetricName
scores O
. O

As O
such O
, O
utilizing O
a O
pre O
- O
trained O
sentence O
embedder O
, O
such O
as O
MPNet B-MethodName
, O
with O
a O
pre O
- O
train O
task O
more O
optimized O
for O
semantic O
embeddings O
results O
in O
better O
performance O
. O

We O
observe O
a O
similar O
trend O
when O
utilizing O
our O
derived O
ruleset O
for O
CAD B-DatasetName
. O

Note O
: O
When O
trained O
longer O
, O
the O
bias O
of O
the O
BERT B-MethodName
model O
decreases O
as O
more O
varied O
sentence O
representations O
are O
learned O
. O

On O
Jigsaw B-DatasetName
and O
Contextual B-DatasetName
Abuse I-DatasetName
datasets I-DatasetName
using O
the O
Hate+Abuse O
List O
and O
derived O
CAD B-DatasetName
Ruleset O
, O
RBE B-MethodName
outperforms O
SOTA O
by O
an O
increased O
margin O
of O
4.1 O
/ O
2.3 B-MetricValue
% I-MetricValue
, O
and O
4.3 O
/ O
1.3 B-MetricValue
% I-MetricValue
respectively O
. O

Contrary O
to O
HateXplain B-DatasetName
, O
these O
two O
datasets O
are O
more O
heavily O
imbalanced O
toward O
non O
- O
hateful O
examples O
and O
thus O
more O
representative O
of O
the O
real O
- O
world O
case O
of O
content O
moderation O
where O
most O
content O
is O
consid O
- O
ered O
benign O
. O

This O
increased O
performance O
highlights O
the O
power O
of O
incorporating O
logical O
rules O
to O
assist O
model O
learning O
and O
also O
the O
ability O
of O
RBE B-MethodName
to O
better O
generalize O
rules O
. O

As O
seen O
in O
Table O
1 O
, O
on O
its O
own O
the O
Hate+Abuse O
ruleset O
performs O
poorly O
on O
each O
dataset O
in O
both O
precision B-MetricName
and O
recall B-MetricName
. O

Despite O
RBE B-MethodName
’s O
reliance O
on O
this O
ruleset O
to O
guide O
model O
learning O
, O
when O
combined O
with O
labeled O
training O
data O
, O
RBE B-MethodName
is O
capable O
of O
both O
restricting O
over O
- O
generalized O
rules O
and O
leveraging O
its O
understanding O
of O
semantic O
similarity O
to O
extend O
fragile O
rules O
regardless O
of O
the O
base O
model O
. O

Additionally O
, O
when O
using O
the O
CAD B-DatasetName
ruleset O
which O
is O
heavily O
overfitted O
to O
the O
CAD B-DatasetName
dataset O
, O
as O
shown O
by O
the O
skewed O
recall B-MetricName
score O
, O
RBE B-MethodName
is O
still O
capable O
of O
outperforming O
the O
baselines O
. O

We O
observe O
that O
even O
when O
applying O
RBE B-MethodName
with O
the O
Hate+Abuse O
ruleset O
we O
are O
able O
to O
outperform O
the O
baselines O
on O
each O
dataset O
. O

When O
applying O
RBE B-MethodName
to O
new O
domain O
settings O
, O
all O
that O
is O
required O
is O
the O
authoring O
of O
additional O
rules O
for O
this O
new O
domain O
. O

4.2 O
Interpretability O
In O
addition O
to O
its O
improved O
performance O
, O
another O
advantage O
of O
RBE B-MethodName
lies O
in O
its O
ability O
to O
perform O
Rule O
- O
grounding O
. O

RBE B-MethodName
is O
seamlessly O
capable O
of O
addressing O
these O
concerns O
by O
facilitating O
rule O
- O
guided O
learning O
. O

By O
defining O
a O
new O
rule O
and O
adding O
at O
least O
one O
exemplar O
, O
RBE B-MethodName
is O
able O
to O
capture O
emerging O
content O
without O
the O
need O
for O
re O
- O
training O
. O

Additionally O
, O
users O
of O
RBE B-MethodName
can O
easily O
modify O
existing O
rules O
that O
may O
be O
too O
broad O
and O
add O
additional O
exemplars O
to O
further O
refine O
predictions O
in O
a O
controllable O
manner O
. O

We O
observe O
that O
RBE B-MethodName
is O
able O
to O
outperform O
SOTA O
trained O
on O
noisy O
rules O
labeled O
samples O
for O
the O
HateXplain B-DatasetName
and O
Jigsaw B-DatasetName
dataset O
while O
also O
outperforming O
the O
ruleset O
as O
is O
on O
all O
three O
datasets O
. O

Across O
each O
dataset O
, O
we O
find O
that O
RBE B-MethodName
’s O
Distance O
based O
strategy O
produces O
the O
most O
consistent O
performance O
, O
outperforming O
SOTA O
on O
HateXplain B-DatasetName
andCAD B-DatasetName
while O
performing O
on O
par O
with O
SOTA O
on O
Jigsaw B-DatasetName
. O

This O
is O
particularly O
useful O
in O
cases O
where O
precision B-MetricName
scores O
are O
low O
due O
to O
a O
large O
number O
of O
false O
positives O
. O

For O
Jigsaw B-DatasetName
, O
we O
observe O
a O
slight O
decrease O
in O
performance O
compared O
to O
SOTA O
. O

Upon O
further O
analysis O
, O
we O
posit O
that O
this O
is O
a O
result O
of O
RBE B-MethodName
’s O
overreliance O
on O
the O
ruleset O
in O
this O
setting O
, O
particularly O
for O
the O
Mean O
andConcat O
strategies O
. O

As O
such O
when O
the O
ruleset O
is O
over O
- O
generalized O
, O
as O
is O
the O
case O
of O
Hate+Abuse O
rules O
on O
Jigsaw B-DatasetName
, O
RBE B-MethodName
is O
likely O
to O
match O
the O
distribution O
of O
the O
ruleset O
. O

As O
such O
, O
with O
a O
more O
refined O
ruleset O
, O
a O
performance O
increase O
is O
expected O
as O
seen O
in O
HateXplain B-DatasetName
and O
CAD B-DatasetName
. O

5 O
Related O
Work O
There O
has O
been O
active O
work O
on O
detecting B-TaskName
hate I-TaskName
speech I-TaskName
in I-TaskName
language I-TaskName
( O
Poletto O
et O
al O
. O
, O
2021 O
; O
AlMakhadmeh O
and O
Tolba O
, O
2020 O
; O
Schmidt O
and O
Wie-371gand O
, O
2017 O
) O
. O

Hate B-TaskName
Speech I-TaskName
detection I-TaskName
has O
proven O
to O
be O
a O
nuanced O
and O
difficult O
task O
, O
leading O
to O
the O
development O
of O
approaches O
and O
datasets O
targeted O
at O
various O
aspects O
of O
the O
problem O
( O
Vidgen O
et O
al O
. O
, O
2021 O
; O
Mathew O
et O
al O
. O
, O
2020 O
; O
Mody O
et O
al O
. O
, O
2023 O
) O
. O

However O
, O
this O
requires O
defining O
rules O
for O
all O
output O
classes O
, O
making O
it O
inapplicable O
to O
the O
task O
of O
hate B-TaskName
speech I-TaskName
detection I-TaskName
. O

6 O
Conclusion O
We O
introduce O
Rule B-MethodName
By I-MethodName
Example I-MethodName
, O
an O
exemplar O
- O
based O
contrastive O
learning O
framework O
that O
enables O
learning O
from O
logical O
rules O
for O
accurate O
and O
explainable O
hate B-TaskName
speech I-TaskName
detection I-TaskName
. O

RBE B-MethodName
leverages O
a O
novel O
exemplar O
- O
based O
contrastive O
learning O
objective O
that O
converges O
the O
representations O
of O
rules O
and O
text O
inputs O
of O
similar O
classes O
. O

We O
share O
results O
on O
three O
public O
datasets O
for O
hate B-TaskName
speech I-TaskName
detection I-TaskName
that O
validate O
the O
Rule B-MethodName
By I-MethodName
Example I-MethodName
framework O
can O
not O
only O
vastly O
outperform O
the O
initial O
ruleset O
but O
also O
outperform O
baseline O
SOTA O
classification O
methods O
in O
both O
supervisedand O
unsupervised O
settings O
. O

Moreover O
, O
RBE B-MethodName
enables O
rule O
- O
grounding O
which O
allows O
for O
more O
explainable O
model O
prediction O
benefits O
not O
available O
in O
SOTA O
classification O
methods O
alongside O
additional O
flexibility O
via O
Ruleset O
Adaptation O
. O

7 O
Limitations O
In O
this O
section O
, O
we O
discuss O
some O
of O
the O
limitations O
of O
the O
Rule B-MethodName
by I-MethodName
Example I-MethodName
method O
. O

7.1 O
Dependence O
on O
Supervision O
The O
requirement O
of O
both O
a O
set O
of O
rules O
and O
an O
example O
per O
rule O
in O
our O
Rule B-MethodName
by I-MethodName
Example I-MethodName
method O
means O
that O
some O
amount O
of O
expert O
supervision O
is O
required O
, O
even O
for O
the O
’ O
unsupervised O
’ O
experimental O
setups O
. O

7.2 O
Increased O
Cost O
Compared O
to O
Rules O
Although O
the O
Rule B-MethodName
by I-MethodName
Example I-MethodName
method O
produces O
a O
Dual O
Encoder O
model O
that O
is O
shown O
to O
be O
much O
more O
performant O
than O
the O
ruleset O
it O
is O
derived O
from O
, O
it O
still O
has O
the O
cost O
limitations O
of O
other O
deep O
learning O
methods O
. O

7.3 O
Reliance O
on O
Quality O
Rules O
and O
Exemplars O
Since O
the O
Rule B-MethodName
by I-MethodName
Example I-MethodName
method O
is O
based O
on O
having O
a O
ruleset O
and O
associated O
exemplars O
to O
learn O
from O
, O
the O
quality O
of O
those O
rules O
and O
exemplars O
could O
affect O
downstream O
Dual O
Encoder O
model O
quality O
. O

A O
possible O
future O
extension O
is O
studying O
the O
effect O
of O
rule O
and O
exemplar O
quality O
on O
the O
performance O
of O
the O
derived O
Dual O
Encoder O
model.3728 O
Ethics O
Hate B-TaskName
speech I-TaskName
detection I-TaskName
is O
a O
complex O
task O
. O

The O
Rule B-MethodName
by I-MethodName
Example I-MethodName
method O
can O
potentially O
reduce O
these O
cases O
, O
for O
example O
by O
learning O
a O
better O
rule O
representation O
and O
identifying O
when O
a O
term O
is O
used O
as O
in O
- O
group O
speech O
as O
opposed O
to O
being O
used O
as O
an O
insult O
or O
slur O
. O


We O
perform O
our O
analyses O
with O
two O
public O
datasets O
from O
the O
World B-DatasetName
Values I-DatasetName
Survey I-DatasetName
( O
across O
55 O
countries O
) O
and O
PEW B-DatasetName
global I-DatasetName
surveys I-DatasetName
( O
across O
40 O
countries O
) O
on O
morality O
. O

For O
example O
, O
with O
no O
supervision O
, O
English B-MethodName
pre I-MethodName
- I-MethodName
trained I-MethodName
language I-MethodName
models I-MethodName
( O
EPLMs B-MethodName
) O
have O
been O
shown O
to O
capture O
people O
’s O
moral O
biases O
and O
distinguish O
between O
morally O
right O
and O
wrong O
actions O
( O
Schramowski O
et O
al O
. O
, O
2022 O
) O
. O

Here O
we O
investigate O
whether O
EPLMs B-MethodName
encode O
knowledge O
about O
moral O
norms O
across O
cultures O
, O
an O
open O
issue O
that O
has O
not O
been O
examined O
comprehensively O
. O

Multilingual B-MethodName
pre I-MethodName
- I-MethodName
trained I-MethodName
language I-MethodName
models I-MethodName
( O
mPLMs B-MethodName
) O
have O
been O
probed O
for O
their O
ability O
to O
identify B-TaskName
cultural I-TaskName
norms I-TaskName
and I-TaskName
biases I-TaskName
in O
a O
restricted O
setting O
( O
Yin O
et O
al O
. O
, O
2022 O
; O
Arora O
et O
al O
. O
, O
2022 O
; O
Hämmerl O
et O
al O
. O
, O
2022 O
; O
Touileb O
et O
al O
. O
, O
2022 O
) O
. O

( O
2022 O
) O
show O
that O
mPLMs B-MethodName
capture O
moral O
norms O
in O
a O
handful O
of O
cultures O
that O
speak O
different O
languages O
. O

However O
, O
it O
remains O
unclear O
whether O
monolingual O
EPLMs B-MethodName
encode O
cultural O
knowledge O
about O
moral O
norms O
. O

Prior O
studies O
have O
only O
used O
EPLMs B-MethodName
to O
assess O
how O
they O
encode O
undesirable O
biases O
toward O
different O
communities O
( O
Ousidhoum O
et O
al O
. O
, O
2021 O
; O
Abid O
et O
al O
. O
, O
2021 O
; O
Sap O
et O
al O
. O
, O
2020 O
; O
Nozza O
et O
al O
. O
, O
2021 O
, O
2022 O
) O
. O

( O
2021 O
) O
show O
that O
GPT3 B-MethodName
can O
generate O
toxic O
comments O
against O
Muslims O
, O
and O
Nozza O
et O
al O
. O

( O
2022 O
) O
explore O
harmful O
text O
generation O
toward O
LGBTQIA+ O
groups O
in O
BERT B-MethodName
models O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

Extending O
these O
lines O
of O
work O
, O
we O
assess O
whether O
monolingual O
EPLMs B-MethodName
can O
accurately O
infer B-TaskName
moral I-TaskName
norms I-TaskName
across I-TaskName
many I-TaskName
cultures I-TaskName
. O

Our O
focus O
on O
EPLMs B-MethodName
is O
due O
partly O
to O
the O
fact O
that O
English O
as O
a O
lingua O
franca O
has O
widespread O
uses O
for O
communication O
in O
- O
person O
and O
through O
online O
media O
. O

Given O
that O
EPLMs B-MethodName
may O
be O
applied O
to O
multicultural O
settings O
, O
it O
is O
important O
to O
understand O
whether O
these O
models O
encode O
basic O
knowledge O
about O
cultural O
diversity O
. O

Another O
motivation O
for O
our O
focus O
is O
that O
while O
it O
is O
expected O
that O
EPLMs B-MethodName
should O
encode O
western O
and428Figure O
1 O
: O
Comparison O
of O
human O
- O
rated O
and O
machine O
- O
scored O
moral O
norms O
across O
cultures O
. O

For O
example O
, O
an O
EPLM B-MethodName
might O
infer O
a O
situation O
to O
be O
morally O
justiﬁable O
( O
e.g. O
, O
“ O
political O
violence O
” O
) O
in O
a O
non O
- O
English O
speaking O
culture O
( O
because O
these O
events O
tend O
to O
associate O
with O
non O
- O
English O
speaking O
cultures O
in O
corpora O
) O
and O
thus O
generate O
misleading O
representations O
of O
that O
community O
. O

Here O
we O
probe O
state O
- O
of O
- O
the O
- O
art O
EPLMs B-MethodName
trained O
on O
large O
English O
- O
based O
datasets O
. O

Using O
EPLMs B-MethodName
also O
supports O
a O
scalable O
analysis O
of O
55 O
countries O
, O
which O
goes O
beyond O
existing O
work O
focusing O
on O
a O
small O
set O
of O
high O
- O
resource O
languages O
from O
mPLMs B-MethodName
and O
monolingual O
PLMs O
. O

We O
take O
the O
moral O
norms O
reported O
in O
different O
countries O
to O
be O
a O
proxy O
of O
cultural O
moral O
norms O
and O
consider O
two O
main O
levels O
of O
analysis O
to O
address O
the O
following O
questions O
: O
•Level O
1 O
: O
Do O
EPLMs B-MethodName
encode O
moral O
knowledge O
that O
mirrors O
the O
moral O
norms O
in O
different O
countries O
? O
For O
example O
, O
“ O
getting O
a O
divorce O
” O
can O
be O
a O
morally O
frowned O
- O
upon O
topic O
in O
country O
i O
, O
but O
morally O
acceptable O
in O
country O
j O
. O

•Level O
2 O
: O
Can O
EPLMs B-MethodName
infer O
the O
cultural O
diversity O
and O
shared O
tendencies O
in O
moral O
judgment O
of O
different O
topics O
? O
For O
example O
, O
people O
across O
nations O
might O
agree O
that O
doing O
X O
is O
morally O
wrong O
while O
disagreeing O
in O
theirmoral O
judgment O
toward O
Y O
. O

We O
probe O
EPLMs B-MethodName
using O
two O
publicly O
available O
global O
surveys O
of O
morality O
, O
World B-DatasetName
Values I-DatasetName
Survey I-DatasetName
wave I-DatasetName
7 I-DatasetName
( O
Haerpfer O
et O
al O
. O
, O
2021 O
) O
1 O
( O
WVS B-DatasetName
) O
and O
PEW B-DatasetName
Global I-DatasetName
Attitudes I-DatasetName
survey I-DatasetName
( O
PEW B-DatasetName
) O
( O
Research O
Center O
, O
2014 O
) O
2 O
. O

For O
example O
, O
according O
to O
WVS B-DatasetName
survey O
( O
illustrated O
in O
Figure O
1 O
) O
, O
people O
in O
different O
cultures O
hold O
disparate O
views O
on O
whether O
“ O
having O
casual O
sex O
” O
is O
morally O
acceptable O
. O

We O
also O
explore O
the O
utility O
- O
bias O
trade O
- O
off O
in O
encoding O
the O
knowledge O
of O
cultural O
moral O
norms O
in O
EPLMs B-MethodName
through O
a O
ﬁne O
- O
tuning O
approach O
. O

Similar O
to O
that O
work O
, O
we O
use O
SBERT B-MethodName
through O
bert O
- O
large O
- O
nli O
- O
mean O
- O
tokens O
sentence O
transformer O
model O
and O
use O
topic O
and O
topic O
- O
country O
pairs O
as O
our O
prompts.4This O
model O
is O
built O
on O
top O
of O
the O
BERT O
model O
, O
which O
is O
pre O
- O
trained O
on O
BOOKS O
CORPUS O
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
Wikipedia O
. O

We O
take O
an O
average O
of O
the O
estimated O
moral B-MetricName
scores I-MetricName
for O
allKpair O
statements O
to O
compute O
the O
moral O
score O
of O
the O
input O
. O

MS O
( O
s O
) O
= O
1 O
KK O
/ O
summationdisplay O
i=1MS O
( O
s+ O
i O
, O
s− O
i O
) O
( O
2 O
) O
To O
construct O
the O
baseline O
, O
we O
compute O
the O
homogeneous O
moral B-MetricName
score I-MetricName
of O
a O
topic O
without O
specifying O
the O
country O
in O
the O
prompts O
. O

Using O
prompt O
pairs O
allows O
us O
to O
operationalize O
moral O
polarity O
: O
a O
positive O
moral O
score O
indicates O
that O
on O
average O
the O
EPLM B-MethodName
is O
more O
likely O
to O
generate O
positive O
moral O
judgment O
for O
inputs O
, O
compared O
to O
negative O
moral O
judgment O
. O

We O
use O
GPT2 B-MethodName
( O
117 O
M O
parameters O
) O
, O
GPT2MEDIUM B-MethodName
( O
345 O
M O
parameters O
) O
, O
GPT2 B-MethodName
- I-MethodName
LARGE I-MethodName
( O
774 O
M O
parameters O
) O
, O
and O
GPT3 B-MethodName
( O
denoted O
as O
GPT3PROBS O
, O
175B O
parameters O
) O
6.GPT2 O
is O
trained O
on O
WEBTEXT O
, O
which O
is O
a O
dataset O
of O
webpages O
and O
contains O
very O
few O
non O
- O
English O
samples O
. O

We O
also O
design O
multiple O
- O
choice O
question O
prompts O
to O
leverage O
the O
question O
- O
answering O
capabilities O
of O
GPT3 B-MethodName
( O
denoted O
as O
GPT3 O
- O
QA O
) O
. O

4.1 O
World B-DatasetName
Values I-DatasetName
Survey I-DatasetName
The O
Ethical O
Values O
section O
in O
World B-DatasetName
Values I-DatasetName
Survey I-DatasetName
Wave I-DatasetName
7 I-DatasetName
( O
WVS B-DatasetName
for O
short O
) O
is O
our O
primary O
dataset O
. O

4.2 O
PEW B-DatasetName
2013 I-DatasetName
global I-DatasetName
attitude I-DatasetName
survey I-DatasetName
We O
use O
a O
secondary O
dataset O
from O
PEW O
Research O
Center O
( O
Research O
Center O
, O
2014 O
) O
based O
on O
a O
public O
survey O
in O
2013 O
that O
studied O
global O
moral O
attitudes O
in O
40 O
countries O
toward O
eight O
morally O
- O
related O
topics O
( O
PEW B-DatasetName
for O
short O
) O
. O

We O
normalized O
these O
ratings O
to O
be O
in O
the O
range O
of O
−1 O
to O
1 O
and O
represented O
each O
topic O
- O
country O
pair O
by O
taking O
an O
expected O
value O
of O
all O
the O
responses.4314.3 O
Homogeneous B-DatasetName
moral I-DatasetName
norms I-DatasetName
We O
also O
use O
the O
data O
from O
the O
global O
user O
study O
in O
Schramowski O
et O
al O
. O

We O
refer O
to O
this O
dataset O
as O
“ O
Homogeneous B-DatasetName
norms I-DatasetName
” O
since O
it O
does O
not O
contain O
information O
about O
moral O
norms O
across O
cultures O
. O

5.1 O
Homogeneous O
moral O
norm O
inference O
For O
homogeneous O
moral O
norm O
inference O
, O
we O
compute O
Pearson B-MetricName
correlation I-MetricName
between O
1 O
) O
the O
empirical O
homogeneous O
moral O
ratings O
, O
obtained O
by O
aggregating O
the O
human O
moral O
ratings O
toward O
a O
topic O
from O
all O
countries O
, O
and O
2 O
) O
language O
model O
inferred O
moral O
scores O
, O
estimated O
from O
our O
homogeneous O
probing O
method O
( O
i.e. O
, O
without O
specifying O
country O
in O
prompts O
) O
. O

Figure O
2 O
shows O
the O
results O
on O
World B-DatasetName
Values I-DatasetName
Survey I-DatasetName
( O
n= O
1,028 B-HyperparameterValue
) O
, O
PEW B-DatasetName
survey I-DatasetName
( O
n= O
312 B-HyperparameterValue
) O
, O
and O
the O
Homogeneous B-DatasetName
norms I-DatasetName
datasets I-DatasetName
( O
n= O
100 B-HyperparameterValue
) O
. O

The O
high O
correlation B-MetricName
of O
GPT2 B-MethodName
andGPT3 B-MethodName
moral O
scores O
with O
the O
Homogeneous O
norms O
dataset O
indicate O
that O
our O
methodology O
does O
indeed O
capture O
the O
embedded O
moral O
biases O
in O
these O
models O
, O
with O
similar O
performance O
to O
the O
method O
proposed O
by O
Schramowski O
et O
al O
. O

( O
2022 O
) O
for O
SBERT B-MethodName
( O
r= B-MetricName
0.79 B-MetricValue
) O
, O
and O
higher O
forGPT3 B-MethodName
- I-MethodName
PROBS I-MethodName
( O
r= B-MetricName
0.85 B-MetricValue
) O
. O

The O
moral O
norms O
in O
this O
dataset O
are O
typically O
more O
globally O
agreeable O
( O
e.g. O
, O
You O
should O
not O
kill O
people O
) O
than O
topics O
in O
WVS B-DatasetName
and O
PEW B-DatasetName
. O

As O
expected O
, O
EPLMs B-MethodName
are O
less O
correlated O
with O
WVS B-DatasetName
and O
PEW B-DatasetName
, O
since O
their O
moral O
biases O
are O
derived O
from O
pre O
- O
training O
on O
English O
and O
westernized O
data O
. O

Aggregated O
ratings O
in O
WVS B-DatasetName
and O
PEW B-DatasetName
, O
however O
, O
capture O
a O
more O
global O
view O
toward O
moral O
issues O
, O
which O
are O
also O
morally O
contentious O
( O
e.g. O
, O
“ O
getting O
a O
divorce O
” O
) O
. O

Using O
our O
ﬁne O
- O
grained O
probing O
approach O
described O
in O
Section O
3 O
, O
we O
compute O
Pearson B-MetricName
correlation I-MetricName
between O
EPLMs O
’ O
moral O
scores O
and O
the O
ﬁne O
- O
grained O
moral O
ratings O
from O
the O
ground O
truth O
. O

Each O
sample O
pair O
in O
the O
correlation B-MetricName
test O
corresponds O
to O
1 O
) O
the O
moral O
norms O
estimated O
by O
EPLMs B-MethodName
for O
a O
country O
cand O
a O
topict O
, O
and O
2 O
) O
the O
empirical O
average O
of O
moral O
ratings O
toward O
topic O
t O
from O
all O
the O
participants O
in O
the O
country O
c O
. O

Figure O
3 O
summarizes O
the O
results O
for O
SBERT B-MethodName
, O
GPT2 B-MethodName
- I-MethodName
LARGE I-MethodName
, O
and O
GPT3 B-MethodName
- I-MethodName
PROBS I-MethodName
models O
, O
and O
the O
rest O
of O
the O
models O
are O
shown O
in O
Figure O
7 O
in O
the O
Appendix O
. O

GPT3 B-MethodName
- I-MethodName
QA I-MethodName
andGPT3 B-MethodName
- I-MethodName
PROBS I-MethodName
both O
show O
a O
relatively O
high O
correlation O
with O
the O
cultural O
variations O
of O
moral O
norms O
( O
r= B-MetricName
0.352 B-MetricValue
, O
r= B-MetricName
0.411 B-MetricValue
, O
p O
< O
0.001 O
, O
for O
both O
) O
, O
and O
GPT2 B-MethodName
- I-MethodName
LARGE I-MethodName
achieves O
a O
correlation O
of O
r= B-MetricName
0.207 B-MetricValue
( O
p O
< O
0.001 O
) O
in O
WVS B-DatasetName
wheren= O
1,028 B-HyperparameterValue
. O

The O
correlations B-MetricName
are O
relatively O
better O
for O
PEW B-DatasetName
( O
n= B-HyperparameterName
312 B-HyperparameterValue
) O
withr= O
0.657 O
, O
r= B-MetricName
0.503 B-MetricValue
, O
andr= O
0.468forGPT3 O
- O
QA O
, O
GPT3PROBS O
andGPT2 O
- O
LARGE O
respectively O
. O

These O
results O
show O
that O
EPLMs B-MethodName
have O
captured O
some432knowledge O
about O
the O
moral O
norms O
of O
different O
cultures O
, O
but O
with O
much O
less O
accuracy O
( O
especially O
for O
GPT2 O
andSBERT O
) O
compared O
to O
their O
inference O
of O
English O
moral O
norms O
shown O
in O
the O
previous O
analysis O
. O

In O
addition O
, O
we O
check O
whether O
GPT3 O
’s O
high O
correlation B-MetricName
with O
PEW B-DatasetName
is O
because O
it O
has O
seen O
and O
memorized O
the O
empirical O
data O
. O

However O
, O
the O
scores O
suggested O
by O
GPT3 B-MethodName
text O
generation O
and O
the O
countries O
’ O
rankings O
based O
on O
their O
ratings O
are O
different O
from O
the O
ground O
truth O
data O
. O

5.3 O
Culture O
clustering O
through O
ﬁne O
- O
grained O
moral O
inference O
EPLMs B-MethodName
’ O
ﬁne O
- O
grained O
knowledge O
of O
moral O
norms O
, O
inspected O
in O
the O
previous O
experiment O
, O
might O
be O
more O
accuracte O
for O
western O
cultures O
than O
other O
cultures O
. O

Our O
ﬁndings O
indicate O
that O
EPLMs B-MethodName
contain O
more O
knowledge O
about O
moral O
norms O
of O
the O
Rich O
West O
countries O
as O
opposed O
to O
non O
- O
western O
and O
non O
- O
rich O
countries O
. O

Similarly O
, O
EPLMs B-MethodName
have O
captured O
a O
more O
accurate O
estimation O
of O
the O
moral O
norms O
in O
countries O
located O
in O
Oceania O
, O
North O
America O
, O
and O
Europe O
, O
as O
opposed O
to O
African O
, O
Asian O
, O
and O
South O
American O
countries O
. O

The O
empirical O
moral O
norm O
ratings O
from O
European O
countries O
in O
WVS B-DatasetName
are O
highly O
aligned O
with O
North O
American O
countries O
( O
r= B-MetricName
0.938 B-MetricValue
) O
, O
which O
explains O
why O
their O
moral O
norms O
are O
inferred O
more O
accurately O
than O
non O
- O
English O
speaking O
countries O
. O

Next O
, O
for O
each O
topic O
, O
we O
compare O
the O
z B-MetricName
- I-MetricName
scores I-MetricName
of O
the O
empirical O
moral O
ratings O
with O
the O
z B-MetricName
- I-MetricName
scores I-MetricName
of O
the O
GPT3 B-MethodName
- I-MethodName
PROBS I-MethodName
inferred O
moral O
scores O
, O
using O
MannWhitney B-MetricName
U I-MetricName
rank O
test O
. O

Such O
underlying O
moral O
biases O
, O
speciﬁcally O
toward O
“ O
homosexuality O
” O
might O
stimulate O
the O
generation O
of O
harmful O
content O
and O
stigmatization O
of O
members O
of O
LGBTQ+ O
, O
which O
has O
been O
reported O
in O
BERT O
- O
based O
EPLMs B-MethodName
( O
Nozza O
et O
al O
. O
, O
2022 O
) O
. O

Such O
cultural O
diversities O
for O
each O
topic O
can O
be O
measured O
by O
taking O
the O
standard B-MetricName
deviation I-MetricName
of O
the O
empirical O
moral O
ratings O
across O
different O
countries O
. O

The O
EPLMs B-MethodName
’ O
inferred O
cultural O
diversities O
can O
similarly O
be O
measured O
by O
taking O
the O
standard B-MetricName
deviation I-MetricName
of O
the O
estimated O
ﬁne O
- O
grained O
moral O
scores O
for O
different O
countries O
. O

We O
then O
quantify O
the O
alignment O
between O
the O
two O
using O
Pearson B-MetricName
correlation I-MetricName
. O

Figure O
5 O
shows O
the O
results O
for O
SBERT B-MethodName
, O
GPT2LARGE B-MethodName
, O
GPT3 B-MethodName
- I-MethodName
PROBS I-MethodName
, O
and O
the O
rest O
are O
shown O
in O
Figure O
8 O
in O
the O
Appendix O
. O

None O
of O
the O
correlations B-MetricName
with O
the O
PEW B-DatasetName
survey O
were O
signiﬁcant O
. O

For O
WVS B-DatasetName
, O
SBERT B-MethodName
, O
GPT2 B-MethodName
andGPT2 B-MethodName
- I-MethodName
MEDIUM I-MethodName
exhibited O
a O
signiﬁcant O
correlation O
( O
p O
< O
0.001 O
) O
with O
r= B-MetricName
0.618 B-MetricValue
, O
r= B-MetricName
0.579 B-MetricValue
, O
andr= B-MetricName
0.734respectively B-MetricValue
. O

6 O
Fine O
- O
tuning O
language O
models O
on O
global O
surveys O
Finally O
, O
we O
explore O
the O
utility O
- O
bias O
trade O
- O
off O
in O
encoding O
cultural O
moral O
knowledge O
into O
EPLMs B-MethodName
by O
ﬁne O
- O
tuning O
them O
on O
cross O
- O
cultural O
surveys O
. O

We O
run O
our O
experiments O
on O
GPT2 B-MethodName
, O
which O
our O
results O
suggest O
having O
captured O
minimum O
information O
about O
cultural O
moral O
norms O
compared O
to O
other O
autoregressive O
models O
. O

Table O
8 O
in O
the O
Appendix O
shows O
our O
prompts O
for O
WVS B-DatasetName
and O
PEW B-DatasetName
. O

For O
the O
Random O
strategy O
, O
we O
randomly O
selected O
80 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
ﬁne O
- O
tuning O
data O
for O
training O
the O
model O
. O

For O
our O
Country O
- O
based O
and O
Topic O
- O
based O
strategies O
, O
we O
randomly O
removed O
20 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
countries O
( O
n= B-HyperparameterName
11 B-HyperparameterValue
for O
WVS O
, O
n= B-HyperparameterName
8 B-HyperparameterValue
for O
PEW O
) O
and O
topics O
( O
n= B-HyperparameterName
4for B-HyperparameterValue
WVS O
, O
n= B-HyperparameterName
2for B-HyperparameterValue
PEW O
) O
from O
the O
training O
data O
to O
compose O
the O
evalu-434PEW O
surveyWorld O
V O
alues O
SurveyEstimated O
degree O
of O
cultural O
diversity O
Empirical O
degree O
of O
cultural O
diversity O
Figure O
5 O
: O
Comparison O
between O
the O
degrees O
of O
cultural O
diversities O
and O
shared O
tendencies O
in O
the O
empirical O
moral O
ratings O
and O
language O
- O
model O
inferred O
moral O
scores O
. O

Table O
1 O
shows O
the O
gained O
utilities O
, O
that O
is O
the O
correlation B-MetricName
test O
results O
between O
the O
ﬁne O
- O
grained O
moral O
scores O
inferred O
by O
the O
ﬁne O
- O
tuned O
models O
and O
the O
empirical O
ﬁne O
- O
grained O
moral O
ratings O
. O

For O
both O
WVS B-DatasetName
and O
PEW B-DatasetName
, O
the O
Random O
strategy O
is O
indeed O
the O
best O
as O
each O
country O
and O
topic O
are O
seen O
in O
the O
training O
data O
at O
least O
once O
( O
but O
may O
not O
appear O
together O
as O
a O
pair O
) O
. O

For O
example O
, O
the O
WVS B-DatasetName
and O
PEW B-DatasetName
- O
trained O
models O
with O
Random O
strategy O
gain O
Pearson B-MetricName
’s O
r B-MetricName
values O
of O
0.893 B-MetricValue
, O
and O
0.944respectively B-MetricValue
. O

Moreover O
, O
injecting O
the O
cross O
- O
cultural O
surveys O
into O
EPLMs B-MethodName
might O
introduce O
additional O
social O
biases O
to O
the O
model O
that O
are O
captured O
through O
these O
surveys O
( O
Joseph O
and O
Morgan O
, O
2020 O
) O
. O

In O
addition O
, O
we O
probe O
the O
best O
ﬁne O
- O
tuned O
model O
( O
i.e. O
, O
WVS B-DatasetName
with O
Random O
strategy O
) O
on O
its O
ability O
to O
capture O
the O
moral O
norms O
of O
non O
- O
western O
cultures O
by O
repeating O
the O
experiment O
in O
Section O
5.3 O
. O

The O
results O
in O
Figure O
4 O
show O
that O
the O
ﬁne O
- O
tuned O
GPT2 B-MethodName
performs O
the O
best O
for O
all O
country O
groups O
. O

However O
, O
basic O
ﬁne O
- O
tuning O
proves O
to O
be O
effective O
in O
adapting O
EPLMs B-MethodName
to O
the O
ground O
truth.4357 O
Discussion O
and O
conclusion O
We O
investigated O
whether O
English B-MethodName
pre I-MethodName
- I-MethodName
trained I-MethodName
language I-MethodName
models I-MethodName
contain O
knowledge O
about O
moral O
norms O
across O
many O
different O
cultures O
. O

Our O
analyses O
show O
that O
large O
EPLMs B-MethodName
capture O
moral O
norm O
variation O
to O
a O
certain O
degree O
, O
with O
the O
inferred O
norms O
being O
predominantly O
more O
accurate O
in O
western O
cultures O
than O
non O
- O
western O
cultures O
. O

Our O
ﬁne O
- O
tuning O
analysis O
further O
suggests O
that O
EPLMs B-MethodName
’ O
cultural O
moral O
knowledge O
can O
be O
improved O
using O
global O
surveys O
of O
moral O
norms O
, O
although O
this O
strategy O
reduces O
the O
capacity O
to O
estimate O
the O
English O
moral O
norms O
and O
potentially O
introduces O
new O
biases O
into O
the O
model O
. O

Given O
the O
increasing O
use O
of O
EPLMs B-MethodName
in O
multicultural O
environments O
, O
our O
work O
highlights O
the O
importance O
of O
cultural O
diversity O
in O
automated O
inference O
of O
moral O
norms O
. O

Even O
when O
an O
action O
such O
as O
“ O
political O
violence O
” O
is O
assessed O
by O
an O
EPLM B-MethodName
as O
morally O
inappropriate O
in O
a O
homogeneous O
setting O
, O
the O
same O
issue O
may O
be O
inferred O
as O
morally O
appropriate O
for O
underrepresented O
cultures O
in O
these O
large O
language O
models O
. O

Currently O
, O
it O
is O
not O
clear O
whether O
the O
difference O
between O
EPLMs B-MethodName
’ O
estimated O
moral O
norms O
and O
the O
empirical O
moral O
ratings O
is O
due O
to O
the O
lack O
of O
cultural O
moral O
norms O
in O
the O
pre O
- O
training O
data O
, O
orthat O
the O
cultural O
moral O
norms O
mentioned O
in O
the O
pre O
- O
training O
data O
represent O
the O
perspective O
of O
an O
English O
- O
speaking O
person O
of O
another O
country O
. O


We O
introduce O
MILDecoding B-MethodName
, O
which O
detoxifies O
language O
models O
at O
token O
- O
level O
by O
interpolating O
it O
with O
a O
trained O
multiple B-MethodName
instance I-MethodName
learning I-MethodName
( O
MIL B-MethodName
) O
network O
. O

MIL B-MethodName
model O
is O
trained O
on O
a O
corpus O
with O
a O
toxicity O
label O
for O
each O
text O
to O
predict O
the O
overall O
toxicity O
and O
the O
toxicity O
of O
each O
token O
in O
its O
context O
. O

Intuitively O
, O
the O
MIL B-MethodName
network O
computes O
a O
toxicity O
distribution O
over O
next O
tokens O
according O
to O
the O
generated O
context O
which O
supplements O
the O
original O
language O
model O
to O
avoid O
toxicity O
. O

We O
evaluate O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
with O
automatic O
metrics O
and O
human O
evaluation O
, O
where O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
outperforms O
other O
baselines O
in O
detoxification B-TaskName
while O
it O
only O
hurts O
generation O
fluency O
a O
little O
bit O
. O

1 O
Introduction O
Trained O
on O
huge O
amount O
of O
text O
corpora O
, O
Transformer B-MethodName
- O
based O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
pretrained O
language O
models O
( O
LMs B-MethodName
) O
have O
led O
to O
a O
wave O
of O
advances O
in O
natural B-TaskName
language I-TaskName
generation I-TaskName
tasks O
( O
Radford O
et O
al O
. O
( O
2019 O
) O
; O
Lewis O
et O
al O
. O
( O
2019 O
) O
; O
Roberts O
et O
al O
. O
( O
2019 O
) O
) O
. O

However O
, O
these O
LMs B-MethodName
are O
capable O
of O
generating O
offensive O
content O
, O
racist O
, O
or O
otherwise O
toxic O
language O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
which O
bring O
security O
risks O
to O
the O
application O
in O
NLP B-TaskName
systems O
. O

We O
examine O
the O
public O
comments O
provided O
in O
Jigsaw B-DatasetName
Toxic I-DatasetName
Comment I-DatasetName
Classification I-DatasetName
Challenge I-DatasetName
Dataset1 I-DatasetName
( O
Jigsaw B-DatasetName
) O
containing O
over O
200 O
K O
comments O
that O
were O
labeled O
as O
toxic O
. O

Approaches O
like O
DEXPERTS B-MethodName
( O
Liu O
et O
al O
. O
, O
2021 O
) O
change O
the O
LM O
output O
distribution O
for O
detoxification O
with O
outside O
expert O
LMs O
, O
making O
it O
hard O
for O
explanation O
. O

Therefore O
, O
we O
present O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
, O
a O
tokenlevel O
detoxification B-TaskName
in O
consideration O
of O
the O
contextual O
information O
with O
a O
multiple B-MethodName
instance I-MethodName
learning I-MethodName
( O
MIL B-MethodName
) O
neural O
network O
. O

At O
each O
decoding O
step O
, O
our O
proposed O
method O
uses O
a O
MIL B-MethodName
network O
to O
score O
the O
retrieved O
tokens O
conditioned O
on O
the O
token O
itself O
and O
its O
contextual O
information O
. O

The O
MIL B-MethodName
network O
predicts O
the O
toxicity O
of O
the O
token O
’s O
occurrence O
in O
the O
generated O
context O
to O
compute O
an O
extra O
toxicity O
distribution O
over O
candidate O
tokens O
to O
avoid O
toxic190generation O
. O

We O
conduct O
experiments O
conditioned O
on O
two O
widely O
- O
used O
datasets O
: O
RealToxicityPrompts B-DatasetName
( O
Gehman O
et O
al O
. O
, O
2020 O
) O
and O
a O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
provided O
by O
Solaiman O
and O
Dennison O
( O
2021 O
) O
. O

Experimental O
results O
show O
that O
our O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
method O
achieves O
faster O
decoding O
speed O
than O
other O
decoding O
- O
time O
methods O
, O
while O
it O
outperforms O
all O
other O
detoxification O
methods O
in O
reducing O
toxic O
text O
generation O
. O

We O
further O
verify O
that O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
can O
mitigate O
toxicity O
conditioned O
on O
either O
nontoxic O
or O
toxic O
prompts O
. O

In O
summary O
, O
the O
contributions O
of O
our O
work O
are O
as O
follows O
: O
•We O
propose O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
that O
introduces O
a O
trained O
MIL B-MethodName
network O
to O
help O
avoid O
toxic O
generation O
. O

•We O
demonstrate O
that O
our O
MIL B-MethodName
network O
can O
help O
analyze O
toxicity O
in O
tokens O
. O

2 O
Background O
2.1 O
Multiple B-MethodName
Instance I-MethodName
Learning I-MethodName
( O
MIL B-MethodName
) O
In O
the O
classical O
supervised O
learning O
problem O
, O
one O
aims O
at O
finding O
a O
model O
that O
predicts O
a O
label O
y O
, O
for O
a O
given O
instance O
x∈RD O
. O

In O
the O
case O
of O
MIL B-MethodName
problem O
, O
however O
, O
one O
deals O
with O
the O
problems O
where O
labels O
are O
associated O
with O
a O
bag O
of O
instances O
, O
X= O
{ O
x1 O
, O
x2 O
, O
x3 O
, O
... O
, O
x O
k O
} O
, O
while O
instance O
labels O
are O
unobserved O
. O

In O
the O
original O
MIL B-MethodName
problem O
settings O
, O
different O
instances O
in O
one O
bag O
exhibit O
neither O
dependency O
nor O
ordering O
among O
each O
other O
. O

MIL B-MethodName
technology O
has O
been O
applied O
to O
sentiment B-TaskName
analysis I-TaskName
( O
Wang O
and O
Wan O
( O
2018 O
) O
; O
Angelidis O
and O
Lapata O
( O
2018 O
) O
) O
, O
and O
we O
propose O
a O
method O
to O
control O
text B-TaskName
generation I-TaskName
with O
it O
. O

Class B-MethodName
- I-MethodName
conditional I-MethodName
language I-MethodName
models I-MethodName
( O
CC B-MethodName
- I-MethodName
LMs I-MethodName
) O
like O
Ctrl B-MethodName
( O
Keskar O
et O
al O
. O
, O
2019b O
) O
guide O
language O
models O
to O
generate O
with O
an O
attribute O
variable O
, O
modeling O
as O
Pθ O
( O
xi|x1 O
: O
i−1 O
, O
c O
) O
, O
where O
variable O
c O
is O
used O
as O
a O
control O
code O
. O

Qian O
et O
al O
. O
( O
2022 O
) O
and O
Clive O
et O
al O
. O
( O
2021 O
) O
introduce O
prefix O
- O
tuning O
in O
steering O
text B-TaskName
generation I-TaskName
with O
a O
control O
prefix O
. O

With O
application O
of O
Bayesian B-MethodName
factorization I-MethodName
, O
the O
problem O
can O
be O
transferred O
into O
maximizing O
the O
product O
of O
Pθ O
( O
xi|x1 O
: O
i−1 O
) O
andPθ O
( O
c|x1 O
: O
i O
) O
: O
Pθ O
( O
x1 O
: O
i|c O
) O
∝Pθ O
( O
xi|x1 O
: O
i−1 O
) O
Pθ O
( O
c|x1 O
: O
i O
) O
( O
1 O
) O
Moreover O
, O
recent O
studies O
further O
paid O
attention O
to O
how O
LMs O
produce O
toxicity O
and O
the O
problems O
with O
existing O
detoxification O
methods O
. O

Moreover O
, O
detoxifying O
LMs O
with O
existing O
methods O
also O
risks O
exacerbating O
bias O
against O
marginalized O
groups O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
. O
Hartvigsen O
et O
al O
. O
( O
2022 O
) O
proposed O
TOXIGEN B-MethodName
, O
an O
extra O
prompt O
dataset O
, O
which O
aims O
to O
help O
mitigate O
the O
bias O
. O

Sridhar O
and O
Yang O
( O
2022 O
) O
introduced O
external O
expert O
knowledge O
to O
help O
enhance O
text O
generation O
models O
to O
explain O
toxicity O
in O
pre O
- O
trained O
LMs O
. O
3 O
Methodology O
The O
core O
idea O
of O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
is O
to O
enhance O
the O
LM O
probability O
distribution O
with O
a O
MIL B-MethodName
network O
that O
computes O
a O
toxicity O
score O
. O

In O
section O
3.1 O
, O
we O
first O
introduce O
the O
MIL B-MethodName
network O
architecture O
and O
analyze O
the O
toxicity O
score O
produced O
by O
the O
network O
. O

And O
then O
, O
we O
provide O
a O
detailed O
description O
of O
our O
approach O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
in O
section O
3.2.191Figure O
1 O
: O
Our O
proposed O
MIL O
network O
. O

The O
model O
encodes O
the O
original O
word O
embedding O
with O
a O
GRU B-MethodName
network O
and O
evaluate O
the O
token O
toxicity O
before O
calculating O
a O
total O
classification O
result O
. O

3.1 O
MIL B-MethodName
Network O
For O
a O
given O
text O
with O
mtokens O
C O
= O
( O
w1 O
, O
w2 O
, O
... O
, O
w O
m O
) O
and O
a O
toxicity O
label O
y∈ O
{ O
0,1 O
} O
, O
the O
MIL O
model O
computes O
the O
toxicity O
of O
each O
token O
, O
and O
predicts O
the O
label O
according O
to O
the O
toxicity O
of O
tokens O
. O

In O
our O
network O
, O
token O
embeddings O
are O
encoded O
with O
a O
bidirectional O
GRU O
layer O
so O
that O
token O
representation O
is O
not O
merely O
based O
on O
the O
token O
itself O
, O
but O
also O
integrates O
contextual O
information O
: O
e1 O
, O
e2 O
, O
... O
, O
e O
m O
= O
GRU B-MethodName
( O
w1 O
, O
w2 O
, O
... O
, O
w O
m O
) O
( O
2 O
) O
Toxicity O
score O
of O
each O
token O
in O
the O
text O
is O
computed O
with O
a O
token O
classification O
module O
containing O
attention O
layers O
and O
activation O
function O
based O
on O
the O
token O
representation O
, O
represented O
by O
function O
f O
: O
p1 O
, O
p2 O
, O
... O
, O
p O
m O
= O
f O
( O
e1 O
, O
e2 O
, O
... O
, O
e O
m O
) O
( O
3 O
) O
Toxicity O
scores O
are O
fed O
into O
a O
document O
classifier O
based O
on O
a O
bidirectional O
GRU B-MethodName
component O
with O
attention O
mechanism O
, O
represented O
by O
function O
g O
: O
ypred O
= O
g O
( O
p1 O
, O
p2 O
, O
... O
, O
p O
m O
) O
( O
4 O
) O
With O
label O
yas O
the O
ground O
truth O
, O
the O
CE O
loss O
between O
ypred O
andyis O
used O
to O
optimize O
the O
MIL B-MethodName
model O
. O

Compared O
with O
previous O
methods O
, O
MIL B-MethodName
network O
learns O
to O
combine O
the O
prior O
toxicity O
probability O
of O
tokens O
and O
its O
contextual O
information O
to O
assign O
toxicity O
score O
for O
each O
token O
. O

Given O
a O
toxic O
sentence O
" O
Tucker O
and O
Paul O
are O
total O
bad O
ass O
mofo O
’s O
. O
" O
, O
it O
is O
tokenized O
by O
a O
GPT-2 B-MethodName
tokenizer O
into O
" O
T O
ucker O
and O
Paul O
are O
total O
bad O
ass O
m O
of O
o O
’s O
. O
< O
eos O
> O
" O
containing O
15 O
tokens O
. O

Each O
token O
is O
assigned O
a O
toxicity O
score O
with O
the O
MIL B-MethodName
model O
. O

After O
studying O
multiple O
toxicity O
score O
outputs O
, O
we O
find O
that O
tokens O
adjacent O
to O
toxic O
spans O
are O
more O
likely O
to O
have O
higher O
toxicity O
score O
due O
to O
the O
influence O
of O
toxic O
context O
and O
properties O
of O
GRU B-MethodName
encoder O
. O

3.2 O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
Detoxification O
Our O
approach O
augments O
a O
pre O
- O
trained O
LM O
with O
the O
MIL B-MethodName
network O
to O
score O
the O
retrieved O
candidate O
tokens O
with O
pre O
- O
trained O
LM O
parameters O
remaining O
unchanged O
. O

At O
inference O
time O
, O
given O
a O
context O
sequence O
of O
tokens O
ct= O
( O
w1 O
, O
w2 O
, O
... O
, O
w O
t−1 O
) O
at O
time O
t O
, O
autoregressive O
LMs O
( O
like O
GPT-2 B-MethodName
) O
estimate O
the O
distribution O
over O
target O
token O
wt O
, O
noted O
as O
PLM O
( O
wt|ct O
) O
. O

We O
adopt O
a O
top- B-MethodName
kfiltering I-MethodName
( O
Fan O
et O
al O
. O
, O
2018 O
) O
method O
that O
preserves O
the O
top O
ktokens O
with O
the O
highest O
probability O
in O
PLM O
( O
wt|ct O
) O
to O
truncate O
the O
unreliable O
tail O
of O
the O
probability O
distribution O
. O

Formally O
, O
letq1 O
, O
q2 O
, O
... O
, O
q O
kdenote O
the O
top- O
kretrieved O
tokens O
at O
time O
t O
, O
the O
MIL B-MethodName
network O
is O
used O
to O
rate O
the O
toxicity O
of O
the O
top- O
kretrieved O
tokens O
by O
concatenating O
each O
candidate O
token O
qito O
the O
context O
ct O
which O
produces O
the O
potential O
generated O
sequence192Figure O
3 O
: O
An O
illustration O
of O
our O
proposed O
detoxification O
via O
multiple O
instance O
learning O
. O

The O
MIL B-MethodName
model O
takes O
ci O
t+1as O
the O
input O
sequence O
and O
assigns O
a O
toxicity O
score O
to O
each O
token O
in O
the O
sequence O
according O
to O
the O
network O
output O
: O
pi O
1 O
, O
pi O
2 O
, O
... O
, O
pi O
t O
= O
f O
( O
GRU O
( O
ci O
t+1 O
) O
) O
( O
5 O
) O
We O
measure O
the O
potential O
toxicity O
of O
token O
qiwith O
the O
output O
score O
pi O
t O
. O

Considering O
the O
sensitivity O
of O
the O
MIL B-MethodName
model O
, O
we O
set O
a O
threhold O
τto O
improve O
generation O
fluency O
. O

The O
last O
step O
is O
to O
interpolate O
the O
toxicity O
distribution O
Ptoxicity O
with O
the O
LM O
distribution O
PLM O
with O
a O
tuned O
hyper O
- O
parameter O
λand O
normalize O
to O
produce O
the O
final O
distribution O
we O
use O
to O
sample O
the O
next O
token O
( O
Khandelwal O
et O
al O
. O
, O
2019 O
) O
: O
P O
( O
y|x O
) O
= O
softmax O
( O
PLM O
( O
y|x O
) O
−λPtox. O
( O
y|x O
) O
) O
( O
6 O
) O
Figure O
3 O
illustrates O
the O
overall O
procedure O
of O
MILDecoding B-MethodName
. O

4 O
Experiments O
We O
use O
GPT-2 B-MethodName
medium O
as O
our O
base O
pre O
- O
trained O
LM O
. O

4.1 O
Baselines O
Domain B-MethodName
- I-MethodName
adaptive I-MethodName
pre I-MethodName
- I-MethodName
training I-MethodName
( O
DAPT B-MethodName
; O
Gururangan O
et O
al O
. O
, O
2020 O
) O
DAPT B-MethodName
attempts O
to O
control O
text O
generation O
by O
finetuning O
pre O
- O
trained O
LMs O
on O
nontoxic O
corpus O
that O
are O
human O
- O
annotated O
. O

However O
, O
DAPT B-MethodName
does O
not O
make O
use O
of O
toxic O
text O
to O
guide O
LMs O
what O
not O
to O
generate O
. O

Using O
the O
same O
training O
data O
as O
our O
proposed O
method O
, O
we O
continue O
pre O
- O
training O
the O
base O
LM O
on O
the O
nontoxic O
corpus O
of O
Jigsaw B-DatasetName
dataset O
which O
contains O
about O
2 O
M O
items O
. O

Plug B-MethodName
- I-MethodName
and I-MethodName
- I-MethodName
Play I-MethodName
language I-MethodName
models I-MethodName
( O
PPLM B-MethodName
; O
Dathathri O
et O
al O
. O
, O
2019 O
) O
PPLM B-MethodName
updates O
the O
hidden O
representation O
with O
gradients O
per O
time O
step O
using O
gradients O
from O
a O
discriminator O
to O
control O
the O
generation O
procedure O
. O

PPLM B-MethodName
steers O
the O
generation O
to O
our O
desirable O
direction O
, O
but O
risk O
hurting O
text O
fluency O
and O
generation O
efficiency O
. O

Generative B-MethodName
discriminator I-MethodName
( O
GeDi B-MethodName
; O
Krause O
et O
al O
. O
, O
2020 O
) O
GeDi B-MethodName
achieves O
strong O
performance O
by O
using O
a O
class B-MethodName
- I-MethodName
conditional I-MethodName
language I-MethodName
model I-MethodName
( O
CC B-MethodName
- I-MethodName
LM I-MethodName
) O
as O
discriminator O
to O
compute O
the O
probability O
contrast O
between O
desired O
control O
code O
and O
anti O
- O
control O
2The O
codes O
are O
available O
at O
https O
: O
/ O
/ O
github.com O
/ O
pkulcwmzx O
/ O
Detoxification193code O
. O

Decoding B-MethodName
- I-MethodName
Time I-MethodName
Controlled I-MethodName
Text I-MethodName
Generation I-MethodName
with I-MethodName
Experts I-MethodName
and I-MethodName
Anti I-MethodName
- I-MethodName
Experts I-MethodName
( O
DEXPERTS B-MethodName
; O
Liu O
et O
al O
. O
, O
2021 O
) O
DEXPERTS B-MethodName
directly O
combines O
probability O
distribution O
from O
an O
expert O
LM O
and O
an O
anti O
- O
expert O
LM O
which O
model O
text O
with O
desirable O
and O
undesirable O
attributes O
. O

DEXPERTS B-MethodName
leverages O
the O
toxic O
corpus O
at O
the O
cost O
of O
introducing O
an O
expert O
and O
an O
anti O
- O
expert O
finetuned O
on O
human O
- O
annotated O
corpus O
. O

RealToxicityPrompts B-DatasetName
( O
Gehman O
et O
al O
. O
, O
2020 O
) O
is O
extracted O
from O
sentences O
in O
OPENWEBTEXT B-DatasetName
CORPUS I-DatasetName
( O
Gokaslan O
and O
Cohen O
, O
2019 O
) O
, O
a O
large O
English O
corpus O
of O
web O
text O
that O
consists O
of O
100 O
K O
prompts O
. O

We O
randomly O
sampled O
10 O
K O
prompts O
from O
RealToxicityPrompts B-DatasetName
for O
evaluation O
, O
since O
the O
test O
time O
of O
some O
baselines O
is O
extremely O
long O
. O

Solaiman O
and O
Dennison O
( O
2021 O
) O
has O
studied O
the O
toxicity O
of O
language O
models O
under O
different O
sensitive O
topics O
with O
a O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
containing O
question O
prompts O
for O
evaluation O
using O
question O
- O
answer O
format O
. O

Organized O
in O
the O
question O
- O
answer O
format O
, O
the O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
contains O
a O
variety O
of O
sensitive O
topics O
that O
can O
induce O
various O
potential O
toxicity O
. O

Since O
the O
QA O
- O
dataset O
is O
relatively O
small O
compared O
with O
RealToxicityPrompts B-DatasetName
, O
we O
use O
it O
to O
assist O
evaluating O
detoxification O
methods O
associated O
with O
sensitive O
topics O
. O

We O
calculate O
two O
metrics O
based O
on O
the O
output O
of O
LM O
: O
1 O
) O
expected B-MetricName
maximum I-MetricName
toxicity I-MetricName
, O
the O
highest O
average O
toxicity O
score O
over O
n O
= O
25 O
generations O
( O
Exp O
. O
Max O
. O
Toxicity O
) O
, O
and O
2 O
) O
the B-MetricName
empirical I-MetricName
probability I-MetricName
of O
generating O
a O
continuation O
with O
toxicity≥0.5 O
at O
least O
once O
over O
n O
= O
25 O
generations O
( O
Toxicity O
Prob O
. O
) O
. O

Generation O
fluency O
and O
diversity O
are O
measured O
using O
the O
mean O
perplexity O
( O
Brown O
et O
al O
. O
, O
1992 O
) O
of O
generated O
continuations O
and O
the B-HyperparameterName
mean I-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
distinct I-HyperparameterName
n I-HyperparameterName
- I-HyperparameterName
grams I-HyperparameterName
as O
in O
the O
previous O
research O
( O
Liu O
et O
al O
. O
, O
2021 O
) O
among O
n O
= O
25 B-HyperparameterValue
generations O
for O
each O
prompt O
. O

4.4 O
Implementation O
Details O
Comments O
in O
Jigsaw B-DatasetName
dataset O
are O
filtered O
by O
token O
number O
, O
reserving O
only O
those O
between O
5 O
and O
200 O
in O
length O
. O

We O
train O
the O
MIL B-MethodName
network O
on O
the O
filtered O
Jigsaw B-DatasetName
dataset O
which O
contains O
about O
2 O
M O
nontoxic O
items O
and O
250 O
K O
toxic O
items O
for O
around O
65 O
hours O
. O

We O
use O
the O
interpolation B-HyperparameterName
weight I-HyperparameterName
λ= O
2.5and B-HyperparameterValue
the O
filter B-HyperparameterName
threhold I-HyperparameterName
τ= O
0.1for B-HyperparameterValue
our O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
generation O
. O

4.5 O
Main O
Results O
Table O
2 O
illustrates O
main O
experimental O
results O
on O
RealToxicityPrompts B-DatasetName
. O

MIL B-MethodName
- I-MethodName
Decoding I-MethodName
is O
more O
time O
efficient O
than O
all O
other O
decoding O
time O
baselines O
, O
only O
a O
little O
slower O
than O
3https O
: O
/ O
/ O
github.com O
/ O
conversationai O
/ O
perspectiveapi194ModelToxicity O
( O
↓ O
) O
Fluency O
( O
↓ O
) O
Diversity O
( O
↑ O
) O
Exp O
. O
Max O
. O
Toxicity O
Toxicity O
Prob O
. O
ppl O
. O

Fluency O
is O
measured O
with O
perplexity O
according O
to O
a O
larger O
GPT-2 B-MethodName
model O
GPT-2 B-MethodName
XL I-MethodName
as O
in O
Liu O
et O
al O
. O
, O
2021 O
. O

Toxicity O
is O
evaluated O
with O
a O
10 O
K O
prompts O
sampled O
from O
RealToxicityPrompts B-DatasetName
( O
Gehman O
et O
al O
. O
, O
2020 O
) O
. O

Two O
metrics O
are O
computed O
1 O
) O
the B-MetricName
highest I-MetricName
average I-MetricName
toxicity I-MetricName
score I-MetricName
over O
25 O
generations O
( O
with O
standard O
deviations O
as O
subscripts O
) O
, O
and O
2 O
) O
the B-MetricName
empirical I-MetricName
probability I-MetricName
of O
generating O
at O
least O
1 O
toxic O
continuation O
for O
each O
prompt O
. O

We O
randomly O
sampled O
100 O
prompts O
from O
the O
10 O
K O
subset O
of O
RealToxicityPrompts B-DatasetName
. O

MIL B-MethodName
- I-MethodName
Decoding I-MethodName
outperforms O
the O
baselines O
in O
reducing O
toxic O
generation O
. O

Although O
MILDecoding B-MethodName
does O
not O
break O
the O
trade O
- O
off O
between O
fluency O
and O
toxicity O
, O
it O
provides O
help O
for O
more O
effective O
detoxification O
. O

4.7 O
QA B-DatasetName
- I-DatasetName
Dataset I-DatasetName
Apart O
from O
the O
main O
results O
on O
RealToxicityPrompts B-DatasetName
, O
Table O
5 O
reports O
experimental O
results O
on O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
. O

MIL B-MethodName
- I-MethodName
Decoding I-MethodName
again O
outperforms O
other O
baselines O
in O
toxicity O
avoidance O
at O
the O
expense O
of O
a O
little O
fluency O
. O

Different O
from O
RealToxicityPrompts B-DatasetName
, O
each O
prompt O
in O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
requires O
the O
language O
model O
to O
answer O
a O
question O
closely O
related O
to O
a O
sensitive O
topic O
, O
where O
a O
right O
position O
needs O
to O
be O
chosen O
. O

Therefore O
, O
some O
methods O
that O
work O
well O
on O
RealToxicityPrompts B-DatasetName
like O
DEXPERTS B-MethodName
might O
not O
adapt O
well O
in O
the O
generation O
about O
sensitive O
topics O
. O

Since O
QA B-DatasetName
- I-DatasetName
dataset I-DatasetName
is O
relatively O
small O
, O
we O
mainly O
study O
prompt O
toxicity O
with O
sampled O
RealToxicityPrompts B-DatasetName
in O
this O
section O
. O

We O
study O
the O
average O
continuation O
toxicity O
generated O
by O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
and O
original O
GPT-2 B-MethodName
conditioned O
on O
prompts O
of O
different O
toxicity O
to O
measure O
the O
detoxification O
performance O
of O
our O
proposed O
method O
given O
different O
prompt O
toxicity O
. O

The O
10 O
K O
sampled O
prompts O
are O
classified O
into O
nontoxic O
prompts O
and O
toxic O
prompts O
according O
to O
the O
toxicity B-MetricName
score I-MetricName
given O
by O
the O
Perpective O
API O
. O

Those O
with O
toxicity B-MetricName
score I-MetricName
≥0.5are B-MetricValue
considered O
toxic O
, O
while O
others O
are O
nontoxic O
prompts O
. O

Our O
proposed O
method O
reduces O
toxic O
continuations O
by O
80 O
% O
induced O
by O
either O
toxic O
or O
nontoxic O
prompts O
, O
indicating O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
per O
- O
forms O
well O
conditioned O
on O
both O
toxic O
and O
nontoxic O
prompts O
. O

5.2 O
Case O
Study O
To O
understand O
how O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
steers O
generation O
from O
toxicity O
, O
we O
manually O
examine O
cases O
that O
change O
the O
generation O
results O
. O

Table O
7 O
shows O
two O
examples O
where O
GPT-2 B-MethodName
first O
generates O
a O
continuation O
and O
we O
apply O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
in O
the O
same O
generated O
context O
before O
toxic O
text O
generation O
. O

In O
the O
first O
case O
in O
Table O
7 O
, O
after O
generating O
" O
and O
I O
’ll O
never O
forget O
" O
, O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
successfully O
changes O
the O
direction O
of O
language O
model O
generation O
to O
avoid O
generating O
the O
toxic O
output O
. O

Highlighted O
red O
tokens O
are O
generated O
by O
the O
GPT-2 B-MethodName
baseline O
which O
might O
cause O
toxicity O
, O
while O
green O
tokens O
denote O
how O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
changes O
the O
generated O
outputs O
. O

In O
this O
section O
, O
we O
briefly O
introduce O
other O
techniques O
related O
to O
controllable O
text B-TaskName
generation I-TaskName
. O

7 O
Conclusion O
We O
have O
introduced O
MIL B-MethodName
- I-MethodName
Decoding I-MethodName
, O
which O
can O
detoxify O
pre O
- O
trained O
LMs O
at O
token O
- O
level O
and O
outperform O
other O
methods O
in O
toxicity B-TaskName
mitigation I-TaskName
. O

MIL B-MethodName
model O
still O
suffers O
from O
the O
tradeoff O
between O
detoxification O
effectiveness O
and O
lan-197guage O
model O
quality O
( O
Wang O
et O
al O
. O
, O
2022 O
) O
. O


